{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#ADF88B\">10 x 3 sec audio files to 30 sec audio file </span> </b>\n",
    "\n",
    "This script concatenates 10x3sec audio files into one 30sec audio file. \n",
    "The script uses the <span style=\"color:#ADF88B\"><b> pydub library</b> </span> to manipulate the audio files. \n",
    "\n",
    "\n",
    "## <span style=\"color:#58F2B3\"> <b>Method: </b> Concatenated Clips  </span>\n",
    "\n",
    "An ID column containing the assessment information was added to the data frame. \n",
    "\n",
    "\n",
    "It was then sorted and grouped by the ID column. The audio clips in each group were then concatenated to equally 10x3 audio clips. \n",
    "\n",
    "\n",
    "All the concatenated audio clips were saved to new directories for easy access.\n",
    "\n",
    "\n",
    "\n",
    "Each audio clip is 3 seconds long, while Whisper expects audio clips of 30 seconds or longer. Therefore, Whisper's responses were tested using 30-second audio clips, achieved by concatenating ten 3-second audio clips. Each clip does not correlate, but it might reveal valuable results for Whisper's performance.\n",
    "\n",
    "The Verbatim Tiny and Medium models from the Norwegian National Library (NNL) were used to compare the performance differences between the two, which had a high difference in features. \n",
    "\n",
    "Multiple tests were conducted to evaluate Whisper's response in this scenario:\n",
    "\n",
    "1. <span style=\"color:#00E6DB\"> Grouping by score: Each score was grouped before transcription. Sorted by file name.</span>\n",
    "2. <span style=\"color:#00E6DB\"> Grouping by person: Each person's independent words were grouped, and sorted by score.</span>\n",
    "3. <span style=\"color:#00E6DB\"> Random shuffling of data before transcription.</span>\n",
    "\n",
    "\n",
    "The results were analyzed using Word Error Rate (WER) and Character Error Rate (CER) scores to assess the performance of the models.\n",
    "\n",
    "If looking for outliers this could have proved usefull, considering if the transcription where much longer og contained question marks.\n",
    "\n",
    "\n",
    "It showed promising results from one iteration, but after that, nothing was correct.\n",
    "\n",
    "\n",
    "<span style=\"color:#00C0FF\">#00C0FF </span>\n",
    "<span style=\"color:#00D5F7\">#00D5F7 </span>\n",
    "<span style=\"color:#00E6DB\">#00E6DB </span>\n",
    "\n",
    "<span style=\"color:#58F2B3\">#58F2B3 </span>\n",
    "<span style=\"color:#ADF88B\">#ADF88B </span>\n",
    "<span style=\"color:#F9F871\">#F9F871 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libary imports\n",
    "from prettytable import PrettyTable\n",
    "import self_made_functions as smf\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment # Good for audio manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import whisper\n",
    "import jiwer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatination information functions\n",
    "\n",
    "Used to save, and exstract wanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Part 1 - Generate the audio files ----------- #\n",
    "def concat_audio(df:pd.DataFrame, save_directory:str, s:int=0, e:int=10, wv_path:str = '/talebase/data/speech_raw/teflon_no/speech16khz/', name = ''):\n",
    "    concat_rows = df.iloc[s:e]  # Rows to concatenate\n",
    "    input_lst = list(concat_rows.Word.values) # get input string\n",
    "    \n",
    "    combined_audio = AudioSegment.empty() # Empty audio file to concatenate the audio clips\n",
    "    id_list = [] # List of the speaker IDs\n",
    "    \n",
    "    target_cer = [] # list of cer scores\n",
    "    target_wer = 0\n",
    "    \n",
    "    for row in concat_rows.itertuples():\n",
    "        id_list.append(row.id.split('_')[0])\n",
    "        \n",
    "        audio = AudioSegment.from_file(os.path.join(wv_path, row[1]))\n",
    "        combined_audio += audio\n",
    "        cer_score = (row.pronScores.count('1')/len(row.pronScores.split(' ')))\n",
    "        target_cer.append(cer_score)\n",
    "        if cer_score != 0:\n",
    "            target_wer += 1   \n",
    "    target_wer = target_wer/len(target_cer)\n",
    "    \n",
    "    audio_name = name + f'words_{s}_{e}.wav'\n",
    "    # print('in funttion', audio_name)\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        print(f\"Directory {save_directory} created\")\n",
    "\n",
    "    # For store filer! Ikke lagre dem direkte i git repo, men på serveren\n",
    "    audio_directory = os.path.join(save_directory, audio_name)\n",
    "    # Save the combined audio\n",
    "    if not os.path.exists(audio_directory):\n",
    "        combined_audio.export(audio_directory, format=\"wav\")   \n",
    "    \n",
    "    return audio_name, id_list, input_lst, target_cer, target_wer\n",
    "\n",
    "# Create dir if not exist\n",
    "def new_dir(dir_name): \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        print(f\"Directory {dir_name} created\")\n",
    "    else:\n",
    "        print(f\"Directory {dir_name} already exists\")\n",
    "\n",
    "# -------------- Part 2 - Look at the result -------------- #\n",
    "def get_transcribed_words(string_words:str):\n",
    "    # Return a list with lower case words and no dots\n",
    "    split_string = string_words.split(' ')\n",
    "    remove_empty = [word for word in split_string if word != '']\n",
    "    lower = [word.lower() for word in remove_empty] \n",
    "    no_dot = [word.replace('.', '') for word in lower]\n",
    "    return no_dot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin, wv_path = smf.get_correct_df()\n",
    "# Speaker ID column added\n",
    "df_trail =  df_fin\n",
    "df_trail['id'] = df_fin['File name'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Prosody</th>\n",
       "      <th>Noise/Disruption</th>\n",
       "      <th>Pre-speech noise</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Word</th>\n",
       "      <th>Pronunciation</th>\n",
       "      <th>pronScores</th>\n",
       "      <th>Assessor</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01_artist.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>A t` I s t</td>\n",
       "      <td>1 0 1 1 1</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01_barn.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>barn</td>\n",
       "      <td>b A: n`</td>\n",
       "      <td>1 0 1</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01_bart.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bart</td>\n",
       "      <td>b A t`</td>\n",
       "      <td>1 1 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01_bjoern.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bjørn</td>\n",
       "      <td>b j 2: n`</td>\n",
       "      <td>1 1 0 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01_blaa.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blå</td>\n",
       "      <td>b l o:</td>\n",
       "      <td>1 1 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File name  Score  Prosody  Noise/Disruption  Pre-speech noise  \\\n",
       "0  a01_artist.wav      4      NaN               NaN               NaN   \n",
       "1    a01_barn.wav      3      1.0               NaN               NaN   \n",
       "2    a01_bart.wav      4      NaN               NaN               NaN   \n",
       "3  a01_bjoern.wav      3      NaN               NaN               NaN   \n",
       "4    a01_blaa.wav      4      NaN               NaN               NaN   \n",
       "\n",
       "   Repetition    Word Pronunciation pronScores    Assessor   id  \n",
       "0         NaN  artist    A t` I s t  1 0 1 1 1  Anne Marte  a01  \n",
       "1         NaN    barn       b A: n`      1 0 1  Anne Marte  a01  \n",
       "2         NaN    bart        b A t`      1 1 0  Anne Marte  a01  \n",
       "3         NaN   bjørn     b j 2: n`    1 1 0 0  Anne Marte  a01  \n",
       "4         NaN     blå        b l o:      1 1 0  Anne Marte  a01  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trail.head()\n",
    "# cer = (df_trail.pronScores[0].count('1')/len(df_trail.pronScores[0].split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00C0FF\"> <b>1. Grouping by score:</b> Each score was grouped before transcription. Sorted by speaker</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_score_sort(model_name):\n",
    "        # Group by score, sort by speaker ID\n",
    "        df_trail_1 = df_trail.sort_values(by=['id'], ascending=False) # Sorted by ID\n",
    "        df_trail_1 = df_trail_1.reset_index(drop=True)\n",
    "        df_score_groups = df_trail_1.groupby('Score')# Grouped by score\n",
    "        \n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "                'score', 'input_string', 'translated_string',\n",
    "                'translated_CER', 'translated_WER', 'target_CER', 'target_CER_sum', 'target_WER',\n",
    "                \"unique_id's\", \"speaker_id's\",\n",
    "                'length_deviation_words',\n",
    "                'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../3x10_Concatenations' # Save outside of git repo\n",
    "\n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "        for idx, score_group in df_score_groups:\n",
    "                # if idx > 1:\n",
    "                #         print('out at', idx)\n",
    "                #         break\n",
    "                print(idx)\n",
    "        # -------------- Make or Find directory -------------- #\n",
    "                score_directory = f'{ground_folder}/3x10_score_{idx}_sorted'\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(score_group)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(score_group, score_directory, s, e)\n",
    "                        unique_ids = list(set(ids))\n",
    "                        \n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(score_directory,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                                        # print(input_words_list[i], translation_list[i], cer) \n",
    "                                        # wer = jiwer.wer(input_words_list[i], translation_list[i])\n",
    "                                        # trans_wer.append(wer)\n",
    "                                # print(input_str)\n",
    "                                # print(translation['text'])\n",
    "                                # print('CER:', trans_cer)\n",
    "                                # print('target CER', target_cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        new_row = {'audio_path': [score_directory],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [idx],\n",
    "                                'unique_id\\'s': [unique_ids],\n",
    "                                'speaker_id\\'s': [ids],\n",
    "                                'target_CER' : [target_cer],\n",
    "                                'target_CER_sum' : [np.sum(target_cer)],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'trans_CER':[trans_cer], \n",
    "                                'trans_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = './3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_scores_id_sorted'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00C0FF\"> <b>1.1 Grouping by score:</b> Each score was grouped before transcription. Randomized speakers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_score_mixed(model_name):\n",
    "        # Group by score, sort by speaker ID\n",
    "        df_trail_2 = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "        df_score_groups = df_trail_2.groupby('Score')# Grouped by score\n",
    "        \n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "                'score', 'input_string', 'translated_string',\n",
    "                'translated_CER', 'translated_WER', 'target_CER', 'target_CER_sum', 'target_WER',\n",
    "                \"unique_id's\", \"speaker_id's\",\n",
    "                'length_deviation_words',\n",
    "                'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../3x10_Concatenations' # Save outside of git repo\n",
    "\n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "        for idx, score_group in df_score_groups:\n",
    "                # if idx > 1:\n",
    "                #         print('out at', idx)\n",
    "                #         break\n",
    "                print(idx)\n",
    "        # -------------- Make or Find directory -------------- #\n",
    "                score_directory = f'{ground_folder}/3x10_score_{idx}_mixed'\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(score_group)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(score_group, score_directory, s, e)\n",
    "                        unique_ids = list(set(ids))\n",
    "                        \n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(score_directory,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                                        # print(input_words_list[i], translation_list[i], cer) \n",
    "                                        # wer = jiwer.wer(input_words_list[i], translation_list[i])\n",
    "                                        # trans_wer.append(wer)\n",
    "                                # print(input_str)\n",
    "                                # print(translation['text'])\n",
    "                                # print('CER:', trans_cer)\n",
    "                                # print('target CER', target_cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        new_row = {'audio_path': [score_directory],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [idx],\n",
    "                                'unique_id\\'s': [unique_ids],\n",
    "                                'speaker_id\\'s': [ids],\n",
    "                                'target_CER' : [target_cer],\n",
    "                                'target_CER_sum' : [np.sum(target_cer)],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'trans_CER':[trans_cer], \n",
    "                                'trans_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = './3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_scores_id_mixed'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00D5F7\"><b> 2. Grouping by person:</b> Each person's independent words were grouped, and sorted by score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_id(model_name): \n",
    "        df_name_groups = df_trail.groupby('id')  # Grouped by file name\n",
    "\n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "        \"speaker_id\", 'score', 'input_string', 'translated_string',\n",
    "        'translated_CER', 'translated_WER', 'target_CER', 'target_CER_sum', 'target_WER',\n",
    "\n",
    "        'length_deviation_words',\n",
    "        'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../3x10_Concatenations/3x10_id_sorted' # Save outside of git repo\n",
    "\n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_name = 'tiny'      \n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "        for idx, person in df_name_groups:\n",
    "                # print(person.iloc[0]['id'])\n",
    "                # if person.iloc[0]['id'] == 'a06':\n",
    "                #         break\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(person)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        speaker_id = person.iloc[0]['id']\n",
    "                        audio_name, _, input_words_list, target_cer, target_wer = concat_audio(person, ground_folder, s, e, name=f'{speaker_id}_')\n",
    "                        # print(audio_name)\n",
    "                        \n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(ground_folder,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        scores = person.iloc[s:e]['Score'].values\n",
    "                        \n",
    "                        new_row = {'audio_path': [ground_folder],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [scores],\n",
    "                                'speaker_id': [speaker_id],\n",
    "                                'target_CER' : [target_cer],\n",
    "                                'target_CER_sum' : [np.sum(target_cer)],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'trans_CER':[trans_cer], \n",
    "                                'trans_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        # print(new_row)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = './3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_by_id'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00E6DB\"><b> 3. Random shuffling of data before transcription.</span></b >\n",
    "\n",
    "Persons not sorted, and results not grouped by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffel_concat(model_name):\n",
    "        # Group by file name, sort by speaker ID\n",
    "        df_no_group = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "        'score', 'input_string', 'translated_string',\n",
    "        'translated_CER', 'translated_WER', 'target_CER', 'target_CER_sum', 'target_WER',\n",
    "        \"unique_id's\", \"speaker_id's\",\n",
    "        'length_deviation_words',\n",
    "        'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../3x10_Concatenations/3x10_no_group_mixed' # Save outside of git repo\n",
    "        \n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "# --------- Concatenate and save audio files --------- #\n",
    "        for i in range(0, len(df_no_group)//10,10):\n",
    "                # Concatenate the 10 audio files together\n",
    "                s, e = i, i + 10 # get start and end index\n",
    "                audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(df_no_group, ground_folder, s, e)\n",
    "                unique_ids = list(set(ids))\n",
    "                \n",
    "# ---------- Transcribe the audio file ---------- #\n",
    "                load_audio = whisper.load_audio(os.path.join(ground_folder,audio_name))\n",
    "                translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                \n",
    "                # calcualte deviation in str length\n",
    "                translation_list = get_transcribed_words(translation['text'])\n",
    "                length_deviation = len(translation_list) - len(input_words_list)\n",
    "                \n",
    "                # Calculate cer and wer with jiwter\n",
    "                input_str = ' '.join(input_words_list)\n",
    "                translation_str = ' '.join(translation_list)\n",
    "                # print(input_str)\n",
    "                # print(translation['text'])\n",
    "                # print(translation_str)\n",
    "                trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                \n",
    "                if len(input_words_list) == len(translation_list):\n",
    "                        trans_cer = []\n",
    "                        for i in range(len(input_words_list)):\n",
    "                                cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                trans_cer.append(cer)\n",
    "                \n",
    "# --------- Store information in data frame --------- #\n",
    "                # add to dataframe\n",
    "                scores = df_no_group.iloc[s:e]['Score'].values\n",
    "                \n",
    "                new_row = {'audio_path': [ground_folder],\n",
    "                        'audio_name': [audio_name],\n",
    "                        \n",
    "                        'score': [scores],\n",
    "                        'unique_id\\'s': [unique_ids],\n",
    "                        'speaker_id\\'s': [ids],\n",
    "                        'target_CER' : [target_cer],\n",
    "                        'target_CER_sum' : [np.sum(target_cer)],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                        'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                        'input_string': [input_str],\n",
    "                        \n",
    "                        'trans_CER':[trans_cer], \n",
    "                        'trans_WER':[trans_wer], \n",
    "                        'translated_string': [translation_str],# [translation],\n",
    "                        \n",
    "                        'length_deviation_words' : [length_deviation]\n",
    "                }\n",
    "                new_row = pd.DataFrame(new_row, index=[0])\n",
    "                # print(new_row.columns.values)\n",
    "                concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = './3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_no_group_mixed'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#58F2B3\"> <b>Results:</b>  for the different tests </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "in funttion words_0_10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3370164/97129814.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in funttion words_10_20.wav\n",
      "2\n",
      "in funttion words_0_10.wav\n",
      "in funttion words_10_20.wav\n",
      "in funttion words_20_30.wav\n",
      "in funttion words_30_40.wav\n",
      "in funttion words_40_50.wav\n",
      "in funttion words_50_60.wav\n",
      "in funttion words_60_70.wav\n",
      "in funttion words_70_80.wav\n",
      "3\n",
      "in funttion words_0_10.wav\n",
      "in funttion words_10_20.wav\n",
      "in funttion words_20_30.wav\n",
      "in funttion words_30_40.wav\n",
      "in funttion words_40_50.wav\n",
      "in funttion words_50_60.wav\n",
      "in funttion words_60_70.wav\n",
      "in funttion words_70_80.wav\n",
      "in funttion words_80_90.wav\n",
      "in funttion words_90_100.wav\n",
      "in funttion words_100_110.wav\n",
      "in funttion words_110_120.wav\n",
      "in funttion words_120_130.wav\n",
      "in funttion words_130_140.wav\n",
      "in funttion words_140_150.wav\n",
      "in funttion words_150_160.wav\n",
      "in funttion words_160_170.wav\n",
      "in funttion words_170_180.wav\n",
      "in funttion words_180_190.wav\n",
      "in funttion words_190_200.wav\n",
      "in funttion words_200_210.wav\n",
      "in funttion words_210_220.wav\n",
      "in funttion words_220_230.wav\n",
      "in funttion words_230_240.wav\n",
      "in funttion words_240_250.wav\n",
      "in funttion words_250_260.wav\n",
      "in funttion words_260_270.wav\n",
      "in funttion words_270_280.wav\n",
      "4\n",
      "in funttion words_0_10.wav\n",
      "in funttion words_10_20.wav\n",
      "in funttion words_20_30.wav\n",
      "in funttion words_30_40.wav\n",
      "in funttion words_40_50.wav\n",
      "in funttion words_50_60.wav\n",
      "in funttion words_60_70.wav\n",
      "in funttion words_70_80.wav\n",
      "in funttion words_80_90.wav\n",
      "in funttion words_90_100.wav\n",
      "in funttion words_100_110.wav\n",
      "in funttion words_110_120.wav\n",
      "in funttion words_120_130.wav\n",
      "in funttion words_130_140.wav\n",
      "in funttion words_140_150.wav\n",
      "in funttion words_150_160.wav\n",
      "in funttion words_160_170.wav\n",
      "in funttion words_170_180.wav\n",
      "in funttion words_180_190.wav\n",
      "in funttion words_190_200.wav\n",
      "in funttion words_200_210.wav\n",
      "in funttion words_210_220.wav\n",
      "in funttion words_220_230.wav\n",
      "in funttion words_230_240.wav\n",
      "in funttion words_240_250.wav\n",
      "5\n",
      "in funttion words_0_10.wav\n",
      "in funttion words_10_20.wav\n",
      "in funttion words_20_30.wav\n",
      "in funttion words_30_40.wav\n",
      "in funttion words_40_50.wav\n",
      "in funttion words_50_60.wav\n",
      "in funttion words_60_70.wav\n",
      "in funttion words_70_80.wav\n",
      "in funttion words_80_90.wav\n",
      "in funttion words_90_100.wav\n",
      "in funttion words_100_110.wav\n",
      "in funttion words_110_120.wav\n",
      "in funttion words_120_130.wav\n",
      "in funttion words_130_140.wav\n",
      "in funttion words_140_150.wav\n",
      "in funttion words_150_160.wav\n",
      "in funttion words_160_170.wav\n",
      "in funttion words_170_180.wav\n",
      "in funttion words_180_190.wav\n",
      "in funttion words_190_200.wav\n",
      "in funttion words_200_210.wav\n",
      "in funttion words_210_220.wav\n",
      "in funttion words_220_230.wav\n",
      "in funttion words_230_240.wav\n",
      "in funttion words_240_250.wav\n",
      "in funttion words_250_260.wav\n",
      "in funttion words_260_270.wav\n",
      "in funttion words_270_280.wav\n",
      "in funttion words_280_290.wav\n",
      "in funttion words_290_300.wav\n",
      "in funttion words_300_310.wav\n",
      "in funttion words_310_320.wav\n",
      "Directory ./3x10_Concatenation_information already exists\n",
      "./3x10_Concatenation_information/tiny__concatenated_audio_information_scores_id_sorted_v1.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_for_score_mixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_name)\n\u001b[1;32m      7\u001b[0m run_for_score_sort(model_name) \u001b[38;5;66;03m#[Done]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mrun_for_score_mixed\u001b[49m(model_name) \u001b[38;5;66;03m# [Not Done]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m group_by_id(model_name)\n\u001b[1;32m     10\u001b[0m random_shuffel_concat(model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_for_score_mixed' is not defined"
     ]
    }
   ],
   "source": [
    "# Test 1 for all the models\n",
    "test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "                    'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "                    'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "for model_name in test_for_models:\n",
    "    print(model_name)\n",
    "    run_for_score_sort(model_name) #[Done]\n",
    "    run_for_score_mixed(model_name) # [Not Done]\n",
    "    group_by_id(model_name)\n",
    "    random_shuffel_concat(model_name)\n",
    "# Kjør gjennom alle csv filene laget og sammen like på vhem som kalrer flest like lange transcribsjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for each model\n",
    "\n",
    "Plot 1 -> Plot over the how many word over and under the target word lengt was.\n",
    "    1. For the 3 experiments for each model\n",
    "    What model had better results when it comes to plotting?\n",
    "Plot 2 -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F9F871\"> Old Code</span>\n",
    "\n",
    "Only tried for global score 5, but for multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00C0FF\"> <b>1.1 Grouping by score:</b> Each score was grouped before transcription. Randomized speakers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by score, randomize id\n",
    "# df_trail_11 = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "# df_score_groups = df_trail_11.groupby('Score')# Grouped by score\n",
    "\n",
    "# concatenated_audio_information = pd.DataFrame(columns=[\n",
    "#     \"audio_path\", \"audio_name\", \"unique_id's\", \"input_string\", \"speaker_id's\"])\n",
    "\n",
    "# for idx, score_group in df_score_groups:\n",
    "# # -------------- Make or Find directory -------------- #\n",
    "#     score_directory = f'3x10_Concatenations/3x10_score_{idx}_random'\n",
    "    \n",
    "# # --------- Concatenate and save audio files --------- #\n",
    "#     for i in range(0, len(score_group)//10,10):\n",
    "#         # Concatenate the 10 audio files together\n",
    "#         s, e = i, i + 10 # get start and end index\n",
    "#         audio_name, ids, input_words = concat_audio(score_group, score_directory, s, e)\n",
    "\n",
    "# # --------- Save audio file info in CSV --------- #\n",
    "#         unique_ids = list(set(ids))\n",
    "#         unique_input_words = np.unique(input_words)\n",
    "        \n",
    "#         # add to dataframe\n",
    "#         new_row = {'audio_path': [score_directory],\n",
    "#                 'audio_name': [audio_name],\n",
    "#                 'unique_id\\'s': [unique_ids],\n",
    "#                 'input_string': [input_words],\n",
    "#                 'speaker_id\\'s': [ids]\n",
    "#                 }\n",
    "        \n",
    "#         new_row = pd.DataFrame(new_row, index=[0])\n",
    "#         concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "\n",
    "# # ----- Save the concatenated audio information ----- #\n",
    "# # concatenated_audio_information.to_csv(f'3x10_Concatenations/concatenated_audio_information_scores_id_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, e = 20, 30\n",
    "new_dir = './10x3sec_audio_files'\n",
    "new_name = '10x3sec_5_rand_word.wav'\n",
    "dire = os.path.join(new_dir, new_name)\n",
    "\n",
    "load_audio = whisper.load_audio(dire)\n",
    "\n",
    "\n",
    "# Only test for the ones with score 5\n",
    "df = df_fin.sort_values(by='Score', ascending=False)\n",
    "audio, ids = concat_audio(df, s, e)\n",
    "# save_audio(audio, new_name, new_dir)\n",
    "\n",
    "print(ids)\n",
    "print(df.iloc[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_made_functions as smf\n",
    "# Tested this for only 5 score files to see if it could work. \n",
    "# If it does not work for the score 5 files, it is unlikely it will work for the lower score files\n",
    "\n",
    "over_view = PrettyTable()\n",
    "over_view.field_names = ['Model','Input length', 'Translated length', 'Word Match', \n",
    "                        'Unique input words', 'Unique translated words',\n",
    "                        'Input words', 'Translated words']\n",
    "\n",
    "test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "                    'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "                    'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "\n",
    "\n",
    "for model_name in test_for_models:\n",
    "    model_path = smf.get_whisper_path(model_name)\n",
    "    model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "    \n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df, s , e)\n",
    "    unique_input_words = df['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    # print(f'Length of translated words : {len(trans_words)}\\n{trans_words}\\n')\n",
    "    # print(f'Length of input words : {len(input_string)}\\n{input_string}\\n')\n",
    "\n",
    "    # print(f'Unique translated words : {unique_words}\\n')\n",
    "    # print(f'Unique input words : {unique_input_words}\\n')\n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "        # The zip function stops creating pairs as soon as one of the input iterables is exhausted,\n",
    "        \n",
    "        over_view.add_row([model_name, len(input_string), len(trans_words), result, unique_input_words, unique_words, input_string, trans_words])\n",
    "    else: \n",
    "        over_view.add_row([model_name, len(input_string), len(trans_words), '', unique_input_words, unique_words, input_string, trans_words])\n",
    "\n",
    "print(over_view)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the table as a CSV file\n",
    "with open('./10x3sec_audio_files/3x10_word_5_random_20_30result_v2.csv', 'w', newline='') as f_output:\n",
    "    f_output.write(over_view.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test show good results for one example for the base,  nb-whisper-medium and  nb-whisper-medium-verbatim.\n",
    "\n",
    "Considering the  nb-whisper-medium-verbatim showed promising results for the other metrics I will try concatenating the rest of teh 5 results for this model.\n",
    "\n",
    "Then calculate the success rate, and deciding if the rest of the scores should be found from this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing only with the nb-whisper-medium-verbatim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_fin[df_fin['Score'] == 5] # new df with only score 5\n",
    "len(df_5)//10 # number of new audio files to make\n",
    "\n",
    "new_dir = './10x3sec_audio_files_5_rand'\n",
    "model_name = 'nb-whisper-medium-verbatim'\n",
    "\n",
    "# Store the information in a data frame\n",
    "over_view_df = pd.DataFrame(columns=['Input length', 'Translated length', 'Word Match',\n",
    "                                    'Unique input words', 'Unique translated words',\n",
    "                                    'Input words', 'Translated words', 'ID'])\n",
    "\n",
    "# Load Model\n",
    "model_path = smf.get_whisper_path(model_name)\n",
    "model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "for i in range(0, len(df_5)//10-10,10):\n",
    "    # Concatenate the 10 audio files together\n",
    "    s, e = i, i+10 # get start and end index\n",
    "    audio, ids = concat_audio(df_5, s, e)\n",
    "    \n",
    "    new_name = f'words_{s}_{e}.wav'\n",
    "    dire = os.path.join(new_dir, new_name)\n",
    "    \n",
    "    # Save the audio and upload using whisper\n",
    "    save_audio(audio, new_name, new_dir)\n",
    "    load_audio = whisper.load_audio(dire)\n",
    "\n",
    "    # Transcribe audio file\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n",
    "    \n",
    "    # Get the words from the transcription adn original data frame\n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) \n",
    "                    if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df_5, s , e)\n",
    "    unique_input_words = df_5['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "    else: \n",
    "      result = ''\n",
    "    \n",
    "    new_df_row = pd.DataFrame(columns=over_view_df.columns)\n",
    "    new_df_row.loc[0] = [len(input_string), len(trans_words), result, unique_input_words, \n",
    "                        unique_words, input_string, trans_words, ids]\n",
    "    over_view_df = pd.concat([over_view_df, new_df_row], ignore_index=True)\n",
    "\n",
    "# save the data frame as a csv in the same directory\n",
    "file_name, version = smf.get_new_csv_name(new_dir, model_name)\n",
    "over_view_df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ser vi at den kan i noen tilfeller transkribere rikgit antall ord, og riktig transcribering uten om noen.\n",
    "\n",
    "Men i fleteparten av tilfellene er det feil antall ord, og feil transcribering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! clip 22 -> 210_220 har en lengre setting \n",
    "\n",
    "Clip 20 med 23 ulike transcriberinger har ikke noen ekstra ord.\n",
    "\n",
    "whisper base vireker helt vilt dårlig, så ser at nb sine modeller er bedre her i alle fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "#                     'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "#                     'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "\n",
    "new_dir = './10x3sec_audio_files_5_rand'\n",
    "model_name = 'base'\n",
    "\n",
    "\n",
    "list_dir = os.listdir(new_dir)\n",
    "wav_files = [file for file in list_dir if file.endswith('.wav')]\n",
    "\n",
    "over_view_df = pd.DataFrame(columns=['Input length', 'Translated length', 'Word Match',\n",
    "                                    'Unique input words', 'Unique translated words',\n",
    "                                    'Input words', 'Translated words', 'ID'])\n",
    "\n",
    "# Load Model\n",
    "model_path = smf.get_whisper_path(model_name)\n",
    "model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "for file in wav_files:\n",
    "    load_audio = whisper.load_audio(os.path.join(new_dir, file))\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n",
    "    s = int(file.split('_')[1])\n",
    "    e = int(file.split('_')[2].split('.')[0])\n",
    "    \n",
    "    # Get the words from the transcription adn original data frame\n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) \n",
    "                    if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df_5, s , e)\n",
    "    unique_input_words = df_5['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "    else: \n",
    "        result = ''\n",
    "    \n",
    "    new_df_row = pd.DataFrame(columns=over_view_df.columns)\n",
    "    new_df_row.loc[0] = [len(input_string), len(trans_words), result, unique_input_words, \n",
    "                        unique_words, input_string, trans_words, '']\n",
    "    over_view_df = pd.concat([over_view_df, new_df_row], ignore_index=True)\n",
    "\n",
    "cvs_file_name, version = smf.get_new_csv_name(new_dir, f'{model_name}')\n",
    "over_view_df.to_csv(cvs_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_glorie.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_loepe.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_krykke.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_skjorte.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_oere.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_oedelagt.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_bryter.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_kvart.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_internett.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_klo.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_krakk.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_brun.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_fjorten.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_port.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_skjerf.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_trapp.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_doer.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_troll.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_trylle.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_kort.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_blomst.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_flue.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_stoevel.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./child_d09_MT'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "csv_df = pd.read_csv('/home/ajtruyen/language_master/child_d09_medium_verbatim.csv')\n",
    "\n",
    "combined_audio = AudioSegment.empty()\n",
    "\n",
    "for row in csv_df.itertuples():\n",
    "    audio_path = os.path.join(wv_path, row[1])\n",
    "    print(audio_path)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    combined_audio += audio\n",
    "               \n",
    "combined_audio.export('./child_d09_MT', format=\"wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/path/to/audio/files/glorie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/audio/files/glorie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(wv_path, row[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(audio_path)  \u001b[38;5;66;03m# Print the path of the audio file being processed\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     combined_audio \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m audio  \u001b[38;5;66;03m# Concatenate the audio\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Export the combined audio as a WAV file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/audio_segment.py:651\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m file, close_file \u001b[38;5;241m=\u001b[39m \u001b[43m_fd_or_path_or_tempfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m:\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:60\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[0;34m(fd, mode, tempfile)\u001b[0m\n\u001b[1;32m     57\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, basestring):\n\u001b[0;32m---> 60\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/audio/files/glorie'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "csv_df = pd.read_csv('/home/ajtruyen/language_master/child_d09_medium_verbatim.csv')\n",
    "\n",
    "# Initialize an empty AudioSegment for combining audio\n",
    "combined_audio = AudioSegment.empty()\n",
    "\n",
    "# Path to the directory containing the audio files\n",
    "wv_path = '/path/to/audio/files'  # Update this path to the actual location\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for row in csv_df.itertuples(index=False):\n",
    "    audio_path = os.path.join(wv_path, row[1])\n",
    "    print(audio_path)  # Print the path of the audio file being processed\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    combined_audio += audio  # Concatenate the audio\n",
    "\n",
    "# Export the combined audio as a WAV file\n",
    "combined_audio.export('./child_d09_MT.wav', format=\"wav\")\n",
    "\n",
    "print(\"Combined audio file has been created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
