{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#ADF88B\">10 x 3 sec audio files to 30 sec audio file </span> </b>\n",
    "\n",
    "This script concatenates 10x3sec audio files into one 30sec audio file. \n",
    "The script uses the <span style=\"color:#ADF88B\"><b> pydub library</b> </span> to manipulate the audio files. \n",
    "\n",
    "\n",
    "## <span style=\"color:#58F2B3\"> <b>Method: </b> Concatenated Clips  </span>\n",
    "\n",
    "An ID column containing the assessment information was added to the data frame. \n",
    "\n",
    "\n",
    "It was then sorted and grouped by the ID column. The audio clips in each group were then concatenated to equally 10x3 audio clips. \n",
    "\n",
    "\n",
    "All the concatenated audio clips were saved to new directories for easy access.\n",
    "\n",
    "\n",
    "\n",
    "Each audio clip is 3 seconds long, while Whisper expects audio clips of 30 seconds or longer. Therefore, Whisper's responses were tested using 30-second audio clips, achieved by concatenating ten 3-second audio clips. Each clip does not correlate, but it might reveal valuable results for Whisper's performance.\n",
    "\n",
    "The Verbatim Tiny and Medium models from the Norwegian National Library (NNL) were used to compare the performance differences between the two, which had a high difference in features. \n",
    "\n",
    "Multiple tests were conducted to evaluate Whisper's response in this scenario:\n",
    "\n",
    "1. <span style=\"color:#00E6DB\"> Grouping by score: Each score was grouped before transcription. Sorted by speaker (?) .</span>\n",
    "2. <span style=\"color:#00E6DB\"> Grouping by person: Each person's independent words were grouped.</span>\n",
    "3. <span style=\"color:#00E6DB\"> Random shuffling of data before transcription.</span>\n",
    "\n",
    "\n",
    "The results were analyzed using Word Error Rate (WER) and Character Error Rate (CER) scores to assess the performance of the models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It showed promising results from one iteration, but after that, nothing was correct.\n",
    "\n",
    "\n",
    "<span style=\"color:#00C0FF\">#00C0FF </span>\n",
    "<span style=\"color:#00D5F7\">#00D5F7 </span>\n",
    "<span style=\"color:#00E6DB\">#00E6DB </span>\n",
    "\n",
    "<span style=\"color:#58F2B3\">#58F2B3 </span>\n",
    "<span style=\"color:#ADF88B\">#ADF88B </span>\n",
    "<span style=\"color:#F9F871\">#F9F871 </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 14:28:01.855018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 14:28:02.962638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Libary imports\n",
    "from prettytable import PrettyTable\n",
    "import self_made_functions as smf\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment # Good for audio manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import whisper\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatination information functions\n",
    "\n",
    "Used to save, and exstract wanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Part 1 - Generate the audio files ----------- #\n",
    "def concat_audio(df:pd.DataFrame, save_directory:str, s:int=0, e:int=10, wv_path:str = '/talebase/data/speech_raw/teflon_no/speech16khz/', name = ''):\n",
    "    concat_rows = df.iloc[s:e]  # Rows to concatenate\n",
    "    input_string = concat_rows['Word'].values # get input string\n",
    "    \n",
    "    combined_audio = AudioSegment.empty() # Empty audio file to concatenate the audio clips\n",
    "    id_list = [] # List of the speaker IDs\n",
    "    \n",
    "    for row in concat_rows.itertuples():\n",
    "        id_list.append(row[1].split('_')[0])\n",
    "        audio = AudioSegment.from_file(os.path.join(wv_path, row[1]))\n",
    "        combined_audio += audio\n",
    "    \n",
    "    audio_name = f'{name}words_{s}_{e}.wav'\n",
    "    audio_directory = os.path.join(save_directory, audio_name)\n",
    "    \n",
    "    # Save the combined audio\n",
    "    combined_audio.export(audio_directory, format=\"wav\")    \n",
    "    return audio_name, id_list, input_string\n",
    "\n",
    "\n",
    "# Create dir if not exist\n",
    "def new_dir(name): \n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)\n",
    "        print(f\"Directory {name} created\")\n",
    "    else:\n",
    "        print(f\"Directory {name} already exists\")\n",
    "\n",
    "# -------------- Part 2 - Look at the result -------------- #\n",
    "def get_transcribed_words(string_words:str):\n",
    "    # Return a list with lower case words and no dots\n",
    "    split_string = string_words.split(' ')\n",
    "    remove_empty = [word for word in split_string if word != '']\n",
    "    lower = [word.lower() for word in remove_empty] \n",
    "    no_dot = [word.replace('.', '') for word in lower]\n",
    "    return no_dot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin, wv_path = smf.get_correct_df()\n",
    "# Speaker ID column added\n",
    "df_trail =  df_fin\n",
    "df_trail['id'] = df_fin['File name'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00C0FF\"> <b>1. Grouping by score:</b> Each score was grouped before transcription. Sorted by speaker</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by score, sort by speaker ID\n",
    "df_trail_1 = df_trail.sort_values(by=['id'], ascending=False) # Sorted by ID\n",
    "df_trail_1 = df_trail_1.reset_index(drop=True)\n",
    "df_score_groups = df_trail_1.groupby('Score')# Grouped by score\n",
    "\n",
    "concatenated_audio_information = pd.DataFrame(columns=[\n",
    "    \"audio_path\", \"audio_name\", \"unique_id's\", \"input_string\", \"speaker_id's\"])\n",
    "\n",
    "for idx, score_group in df_score_groups:\n",
    "# -------------- Make or Find directory -------------- #\n",
    "    score_directory = f'3x10_Concatenations/3x10_score_{idx}_sorted'\n",
    "    new_dir(score_directory)\n",
    "    \n",
    "# --------- Concatenate and save audio files --------- #\n",
    "    for i in range(0, len(score_group)//10,10):\n",
    "        # Concatenate the 10 audio files together\n",
    "        s, e = i, i + 10 # get start and end index\n",
    "        audio_name, ids, input_words = concat_audio(score_group, score_directory, s, e)\n",
    "\n",
    "# --------- Save audio file info in CSV --------- #\n",
    "        unique_ids = list(set(ids))\n",
    "        unique_input_words = np.unique(input_words)\n",
    "        \n",
    "        # add to dataframe\n",
    "        new_row = {'audio_path': [score_directory],\n",
    "                'audio_name': [audio_name],\n",
    "                'unique_id\\'s': [unique_ids],\n",
    "                'input_string': [input_words],\n",
    "                'speaker_id\\'s': [ids]\n",
    "                }\n",
    "        \n",
    "        new_row = pd.DataFrame(new_row, index=[0])\n",
    "        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "\n",
    "# ----- Save the concatenated audio information ----- #\n",
    "concatenated_audio_information.to_csv(f'3x10_Concatenations/concatenated_audio_information_scores_id_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00C0FF\"> <b>1.1 Grouping by score:</b> Each score was grouped before transcription. Randomized speakers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 3x10_Concatenations/3x10_score_1_random created\n",
      "Directory 3x10_Concatenations/3x10_score_2_random created\n",
      "Directory 3x10_Concatenations/3x10_score_3_random created\n",
      "Directory 3x10_Concatenations/3x10_score_4_random created\n",
      "Directory 3x10_Concatenations/3x10_score_5_random created\n"
     ]
    }
   ],
   "source": [
    "# Group by score, randomize id\n",
    "df_trail_11 = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "df_score_groups = df_trail_11.groupby('Score')# Grouped by score\n",
    "\n",
    "concatenated_audio_information = pd.DataFrame(columns=[\n",
    "    \"audio_path\", \"audio_name\", \"unique_id's\", \"input_string\", \"speaker_id's\"])\n",
    "\n",
    "for idx, score_group in df_score_groups:\n",
    "# -------------- Make or Find directory -------------- #\n",
    "    score_directory = f'3x10_Concatenations/3x10_score_{idx}_random'\n",
    "    new_dir(score_directory)\n",
    "    \n",
    "# --------- Concatenate and save audio files --------- #\n",
    "    for i in range(0, len(score_group)//10,10):\n",
    "        # Concatenate the 10 audio files together\n",
    "        s, e = i, i + 10 # get start and end index\n",
    "        audio_name, ids, input_words = concat_audio(score_group, score_directory, s, e)\n",
    "\n",
    "# --------- Save audio file info in CSV --------- #\n",
    "        unique_ids = list(set(ids))\n",
    "        unique_input_words = np.unique(input_words)\n",
    "        \n",
    "        # add to dataframe\n",
    "        new_row = {'audio_path': [score_directory],\n",
    "                'audio_name': [audio_name],\n",
    "                'unique_id\\'s': [unique_ids],\n",
    "                'input_string': [input_words],\n",
    "                'speaker_id\\'s': [ids]\n",
    "                }\n",
    "        \n",
    "        new_row = pd.DataFrame(new_row, index=[0])\n",
    "        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "\n",
    "# ----- Save the concatenated audio information ----- #\n",
    "concatenated_audio_information.to_csv(f'3x10_Concatenations/concatenated_audio_information_scores_id_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00D5F7\"><b> 2. Grouping by person:</b> Each person's independent words were grouped, and sorted by score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 3x10_Concatenations/3x10_id_sorted already exists\n"
     ]
    }
   ],
   "source": [
    "# Group by file name, sort by speaker ID\n",
    "df_name_groups = df_trail.groupby('id')  # Grouped by file name\n",
    "\n",
    "# -------------- Make or Find directory -------------- #\n",
    "file_directory = f'3x10_Concatenations/3x10_id_sorted'\n",
    "new_dir(file_directory)\n",
    "\n",
    "# --------- Concatenate and save audio files --------- #\n",
    "concatenated_audio_information = pd.DataFrame(columns=[\"audio_name\", \"speaker_id\", \"input_string\" \"scores\"])\n",
    "\n",
    "for idx, person in df_name_groups:\n",
    "        # person = person.sort_values(by=['Score'], ascending=False)  # Sorted by ID\n",
    "        for i in range(0, len(person)//10, 10):\n",
    "                # Concatenate the 10 audio files together\n",
    "                start, end = i, i + 10  # get start and end index\n",
    "                speaker_id = ids[0] # The speacker id is the same in the groups\n",
    "                audio_name, ids, input_words = concat_audio(person, file_directory, start, end, name=f'{speaker_id}_')\n",
    "                \n",
    "                # Create audio name with speaker ID prefix\n",
    "                scor_list = person.iloc[start:end]['Score'].values\n",
    "\n",
    "                # Add to DataFrame\n",
    "                new_row = {\n",
    "                        'audio_name': [audio_name],\n",
    "                        'speaker_id': [speaker_id],\n",
    "                        'input_string': [input_words],\n",
    "                        'scores': [scor_list]\n",
    "                        }\n",
    "                new_row = pd.DataFrame(new_row, index=[0])\n",
    "                concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "\n",
    "# ----- Save the concatenated audio information to CSV ----- #\n",
    "concatenated_info_csv = os.path.join(file_directory, 'concatenated_audio_information.csv')\n",
    "concatenated_audio_information.to_csv(concatenated_info_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00E6DB\"><b> 3. Random shuffling of data before transcription.</span></b >\n",
    "\n",
    "Persons not sorted, and results not grouped by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 3x10_Concatenations/3x10_no_group_mixed already exists\n"
     ]
    }
   ],
   "source": [
    "# Group by file name, sort by speaker ID\n",
    "df_no_group = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# -------------- Make or Find directory -------------- #\n",
    "file_directory = f'3x10_Concatenations/3x10_no_group_mixed'\n",
    "new_dir(file_directory)\n",
    "\n",
    "# --------- Concatenate and save audio files --------- #\n",
    "concatenated_audio_information = pd.DataFrame(columns=[\"audio_name\", \"speaker_id\", \"input_string\" \"scores\"])\n",
    "\n",
    "# person = person.sort_values(by=['Score'], ascending=False)  # Sorted by ID\n",
    "for i in range(0, len(df_no_group)//10, 10):\n",
    "        # Concatenate the 10 audio files together\n",
    "        start, end = i, i + 10  # get start and end index\n",
    "        audio_name, ids, input_words = concat_audio(df_no_group, file_directory, start, end)\n",
    "        \n",
    "        # Create audio name with speaker ID prefix\n",
    "        scor_list = df_no_group.iloc[start:end]['Score'].values\n",
    "\n",
    "        # Add to DataFrame\n",
    "        new_row = {\n",
    "                'audio_name': [audio_name],\n",
    "                'speaker_id': [ids],\n",
    "                'input_string': [input_words],\n",
    "                'scores': [scor_list]\n",
    "                }\n",
    "        new_row = pd.DataFrame(new_row, index=[0])\n",
    "        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "\n",
    "# ----- Save the concatenated audio information to CSV ----- #\n",
    "concatenated_info_csv = os.path.join(file_directory, 'concatenated_audio_information.csv')\n",
    "concatenated_audio_information.to_csv(concatenated_info_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#58F2B3\"> <b>Results:</b>  for the different tests </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F9F871\"> Old Code</span>\n",
    "\n",
    "Only tried for global score 5, but for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, e = 20, 30\n",
    "new_dir = './10x3sec_audio_files'\n",
    "new_name = '10x3sec_5_rand_word.wav'\n",
    "dire = os.path.join(new_dir, new_name)\n",
    "\n",
    "load_audio = whisper.load_audio(dire)\n",
    "\n",
    "\n",
    "# Only test for the ones with score 5\n",
    "df = df_fin.sort_values(by='Score', ascending=False)\n",
    "audio, ids = concat_audio(df, s, e)\n",
    "save_audio(audio, new_name, new_dir)\n",
    "\n",
    "print(ids)\n",
    "print(df.iloc[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_made_functions as smf\n",
    "# Tested this for only 5 score files to see if it could work. \n",
    "# If it does not work for the score 5 files, it is unlikely it will work for the lower score files\n",
    "\n",
    "over_view = PrettyTable()\n",
    "over_view.field_names = ['Model','Input length', 'Translated length', 'Word Match', \n",
    "                        'Unique input words', 'Unique translated words',\n",
    "                        'Input words', 'Translated words']\n",
    "\n",
    "test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "                    'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "                    'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "\n",
    "\n",
    "for model_name in test_for_models:\n",
    "    model_path = smf.get_whisper_path(model_name)\n",
    "    model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "    \n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df, s , e)\n",
    "    unique_input_words = df['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    # print(f'Length of translated words : {len(trans_words)}\\n{trans_words}\\n')\n",
    "    # print(f'Length of input words : {len(input_string)}\\n{input_string}\\n')\n",
    "\n",
    "    # print(f'Unique translated words : {unique_words}\\n')\n",
    "    # print(f'Unique input words : {unique_input_words}\\n')\n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "        # The zip function stops creating pairs as soon as one of the input iterables is exhausted,\n",
    "        \n",
    "        over_view.add_row([model_name, len(input_string), len(trans_words), result, unique_input_words, unique_words, input_string, trans_words])\n",
    "    else: \n",
    "        over_view.add_row([model_name, len(input_string), len(trans_words), '', unique_input_words, unique_words, input_string, trans_words])\n",
    "\n",
    "print(over_view)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the table as a CSV file\n",
    "with open('./10x3sec_audio_files/3x10_word_5_random_20_30result_v2.csv', 'w', newline='') as f_output:\n",
    "    f_output.write(over_view.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test show good results for one example for the base,  nb-whisper-medium and  nb-whisper-medium-verbatim.\n",
    "\n",
    "Considering the  nb-whisper-medium-verbatim showed promising results for the other metrics I will try concatenating the rest of teh 5 results for this model.\n",
    "\n",
    "Then calculate the success rate, and deciding if the rest of the scores should be found from this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing only with the nb-whisper-medium-verbatim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_fin[df_fin['Score'] == 5] # new df with only score 5\n",
    "len(df_5)//10 # number of new audio files to make\n",
    "\n",
    "new_dir = './10x3sec_audio_files_5_rand'\n",
    "model_name = 'nb-whisper-medium-verbatim'\n",
    "\n",
    "# Store the information in a data frame\n",
    "over_view_df = pd.DataFrame(columns=['Input length', 'Translated length', 'Word Match',\n",
    "                                    'Unique input words', 'Unique translated words',\n",
    "                                    'Input words', 'Translated words', 'ID'])\n",
    "\n",
    "# Load Model\n",
    "model_path = smf.get_whisper_path(model_name)\n",
    "model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "for i in range(0, len(df_5)//10-10,10):\n",
    "    # Concatenate the 10 audio files together\n",
    "    s, e = i, i+10 # get start and end index\n",
    "    audio, ids = concat_audio(df_5, s, e)\n",
    "    \n",
    "    new_name = f'words_{s}_{e}.wav'\n",
    "    dire = os.path.join(new_dir, new_name)\n",
    "    \n",
    "    # Save the audio and upload using whisper\n",
    "    save_audio(audio, new_name, new_dir)\n",
    "    load_audio = whisper.load_audio(dire)\n",
    "\n",
    "    # Transcribe audio file\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n",
    "    \n",
    "    # Get the words from the transcription adn original data frame\n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) \n",
    "                    if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df_5, s , e)\n",
    "    unique_input_words = df_5['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "    else: \n",
    "      result = ''\n",
    "    \n",
    "    new_df_row = pd.DataFrame(columns=over_view_df.columns)\n",
    "    new_df_row.loc[0] = [len(input_string), len(trans_words), result, unique_input_words, \n",
    "                        unique_words, input_string, trans_words, ids]\n",
    "    over_view_df = pd.concat([over_view_df, new_df_row], ignore_index=True)\n",
    "\n",
    "# save the data frame as a csv in the same directory\n",
    "file_name, version = smf.get_new_csv_name(new_dir, model_name)\n",
    "over_view_df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ser vi at den kan i noen tilfeller transkribere rikgit antall ord, og riktig transcribering uten om noen.\n",
    "\n",
    "Men i fleteparten av tilfellene er det feil antall ord, og feil transcribering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! clip 22 -> 210_220 har en lengre setting \n",
    "\n",
    "Clip 20 med 23 ulike transcriberinger har ikke noen ekstra ord.\n",
    "\n",
    "whisper base vireker helt vilt dårlig, så ser at nb sine modeller er bedre her i alle fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "#                     'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "#                     'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "\n",
    "new_dir = './10x3sec_audio_files_5_rand'\n",
    "model_name = 'base'\n",
    "\n",
    "\n",
    "list_dir = os.listdir(new_dir)\n",
    "wav_files = [file for file in list_dir if file.endswith('.wav')]\n",
    "\n",
    "over_view_df = pd.DataFrame(columns=['Input length', 'Translated length', 'Word Match',\n",
    "                                    'Unique input words', 'Unique translated words',\n",
    "                                    'Input words', 'Translated words', 'ID'])\n",
    "\n",
    "# Load Model\n",
    "model_path = smf.get_whisper_path(model_name)\n",
    "model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "for file in wav_files:\n",
    "    load_audio = whisper.load_audio(os.path.join(new_dir, file))\n",
    "    words = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n",
    "    s = int(file.split('_')[1])\n",
    "    e = int(file.split('_')[2].split('.')[0])\n",
    "    \n",
    "    # Get the words from the transcription adn original data frame\n",
    "    trans_words = get_transcribed_words(words['text'])\n",
    "    unique_words = [unique_word for word, unique_word in enumerate(trans_words) \n",
    "                    if word == trans_words.index(unique_word)]\n",
    "\n",
    "    input_string = get_input_string(df_5, s , e)\n",
    "    unique_input_words = df_5['Word'].iloc[s:e].unique()\n",
    "    \n",
    "    if len(input_string)  == len(trans_words):\n",
    "        # Compare elements and create a new list with 1s and 0s # Phind.com\n",
    "        result = [int(a == b) for a, b in zip(input_string, trans_words)] \n",
    "    else: \n",
    "        result = ''\n",
    "    \n",
    "    new_df_row = pd.DataFrame(columns=over_view_df.columns)\n",
    "    new_df_row.loc[0] = [len(input_string), len(trans_words), result, unique_input_words, \n",
    "                        unique_words, input_string, trans_words, '']\n",
    "    over_view_df = pd.concat([over_view_df, new_df_row], ignore_index=True)\n",
    "\n",
    "cvs_file_name, version = smf.get_new_csv_name(new_dir, f'{model_name}')\n",
    "over_view_df.to_csv(cvs_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_glorie.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_loepe.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_krykke.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_skjorte.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_oere.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_oedelagt.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_bryter.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_kvart.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_internett.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_klo.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_krakk.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_brun.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_fjorten.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_port.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_skjerf.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_trapp.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_doer.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_troll.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_trylle.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_kort.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_blomst.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_flue.wav\n",
      "/talebase/data/speech_raw/teflon_no/speech16khz/d09_stoevel.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./child_d09_MT'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "csv_df = pd.read_csv('/home/ajtruyen/language_master/child_d09_medium_verbatim.csv')\n",
    "\n",
    "combined_audio = AudioSegment.empty()\n",
    "\n",
    "for row in csv_df.itertuples():\n",
    "    audio_path = os.path.join(wv_path, row[1])\n",
    "    print(audio_path)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    combined_audio += audio\n",
    "               \n",
    "combined_audio.export('./child_d09_MT', format=\"wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/path/to/audio/files/glorie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/audio/files/glorie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(wv_path, row[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(audio_path)  \u001b[38;5;66;03m# Print the path of the audio file being processed\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     combined_audio \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m audio  \u001b[38;5;66;03m# Concatenate the audio\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Export the combined audio as a WAV file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/audio_segment.py:651\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 651\u001b[0m file, close_file \u001b[38;5;241m=\u001b[39m \u001b[43m_fd_or_path_or_tempfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m:\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pydub/utils.py:60\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[0;34m(fd, mode, tempfile)\u001b[0m\n\u001b[1;32m     57\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, basestring):\n\u001b[0;32m---> 60\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/audio/files/glorie'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "csv_df = pd.read_csv('/home/ajtruyen/language_master/child_d09_medium_verbatim.csv')\n",
    "\n",
    "# Initialize an empty AudioSegment for combining audio\n",
    "combined_audio = AudioSegment.empty()\n",
    "\n",
    "# Path to the directory containing the audio files\n",
    "wv_path = '/path/to/audio/files'  # Update this path to the actual location\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for row in csv_df.itertuples(index=False):\n",
    "    audio_path = os.path.join(wv_path, row[1])\n",
    "    print(audio_path)  # Print the path of the audio file being processed\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    combined_audio += audio  # Concatenate the audio\n",
    "\n",
    "# Export the combined audio as a WAV file\n",
    "combined_audio.export('./child_d09_MT.wav', format=\"wav\")\n",
    "\n",
    "print(\"Combined audio file has been created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
