{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#D65DB1\"> Use Decision Tree Classifier to give score to the transcribed results </span></b>\n",
    "\n",
    "The innput will be the CER score, and the output is the number 1 to 5.\n",
    "The Clasefier is a Decision Tree from sklearn library.\n",
    "\n",
    "Implementation:\n",
    "1)  Get the CER from the assesment file. \n",
    "    This is calculated using pronScores.\n",
    "2)  Make a new df wher the assesment 'CER' and global 'Score' is.\n",
    "3)  Split the df into traing and testing data.\n",
    "4)  Initilize a Decision Tree Classifier\n",
    "5)  Train the classefier, and test it.\n",
    "6)  Opimize the model if needed\n",
    "7)  Change the predicted Score in the transcribed files and update the CSV and results.\n",
    "\n",
    "For 'Score' 5 where 'CER' is more than 0.0, is one incstand at index 7115. With the CER score of 0.333333.\n",
    "\n",
    "<!-- \n",
    "# Code\n",
    "filtered_df = input_assesmen[(input_assesmen['Score'] == 5) & (input_assesmen['CER'] > 0)]\n",
    "print(\"\\nFiltered rows where 'Score' is 5 and 'CER' is greater than 0:\\n\", filtered_df) \n",
    "->\n",
    "\n",
    "<!-- \n",
    "# Test for forstÃ¥else\n",
    "# df_assesment.loc[0]\n",
    "# print(len(df_assesment['Pronunciation'].loc[0].split(' ')))\n",
    "# print(len(df_assesment['pronScores'].loc[0].split(' ')))\n",
    "# pronScore =  df_assesment['pronScores'].loc[0].split(' ')\n",
    "# CER = pronScore.count('0')/len(df_assesment['pronScores'].loc[0].split(' '))\n",
    "\n",
    "# print(CER)\n",
    "# print(pronScore)\n",
    "# print(pronScore.count('0'))\n",
    "# print(pronScore.count('1'))\n",
    "# # Double check that the len of the pronScore is the same as the len of the Pronunciation\n",
    "# for idx, row in df_assesment.iterrows():\n",
    "#     pron = len(row['Pronunciation'].split(' '))\n",
    "#     pronScor = len(row['pronScores'].split(' '))\n",
    "#     if pron != pronScor:\n",
    "#         print('index', idx)\n",
    "#         print('Pronunciation', pron)\n",
    "#         print('pronScores', pronScor) \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors: \n",
    "\n",
    "<b><span style=\"color:#7ABCDD\">7ABCDD</span></b>\n",
    "<b><span style=\"color:#57D0E2\">57D0E2</span></b>\n",
    "<b><span style=\"color:#4DE1D4\">4DE1D4</span></b>\n",
    "<b><span style=\"color:#77EFB6\">77EFB6</span></b>\n",
    "<b><span style=\"color:#B4F790\">B4F790</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#D4A5CA\">D4A5CA</span></b>\n",
    "<b><span style=\"color:#FBA8C2\">FBA8C2</span></b>\n",
    "<b><span style=\"color:#FFAFAC\">FFAFAC</span></b>\n",
    "<b><span style=\"color:#FFC08F\">FFC08F</span></b>\n",
    "<b><span style=\"color:#FFDA77\">FFDA77</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#D65DB1\">D65DB1</span></b>\n",
    "<b><span style=\"color:#FF6F91\">FF6F91</span></b>\n",
    "<b><span style=\"color:#FF9671\">FF9671</span></b>\n",
    "<b><span style=\"color:#FFC75F\">FFC75F</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#2C73D2\">2C73D2</span></b>\n",
    "<b><span style=\"color:#0081CF\">0081CF</span></b>\n",
    "<b><span style=\"color:#0089BA\">0089BA</span></b>\n",
    "<b><span style=\"color:#008E9B\">008E9B</span></b>\n",
    "<b><span style=\"color:#008F7A\">008F7A</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#FF6F91\"> Problems whit the current methode: </span> \n",
    "\n",
    "The dataset is <b>unbalanced</b>. \n",
    "Instances of 'Score' 4 is higest for the same group as 'Score' 5. \n",
    "\n",
    "'Score' 4 is never the highest number for each of the CER scores, so it will not be used in the classefiing process. \n",
    "\n",
    "This means in the reality that <b>the model will only be able to predict 'Score' 1, 2, 3 and 5</b>, where most of them is the lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 12:41:27.081594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 12:41:27.948894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import self_made_functions as smf \n",
    "import pandas as pd\n",
    "\n",
    "df_assesment, wv_path = smf.get_correct_df()\n",
    "\n",
    "# # File name\tScore\tProsody\tNoise/Disruption\tPre-speech noise\tRepetition\tWord\tPronunciation\tpronScores\tAssessor\n",
    "csv_df  = pd.read_csv('Transcriptions/transcriptions_nb-whisper-medium-verbatim_v1.csv') # Test for 1 file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = df_assesment[df_assesment['File name'] == 'a12_hylle.wav']\n",
    "# pron_lst = row.pronScores.values[0].split(' ')\n",
    "# pron_count = pron_lst.count('0')\n",
    "# cer = pron_count/len(pron_lst)\n",
    "# cer\n",
    "# print(pron_lst, pron_count, len(pron_lst), cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"color:#FFC75F\"> Add ekstra information columns </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df[\"CER Score\"] = float('nan') # Remove the scores since they are not correct calculated\n",
    "csv_df[\"ID\"] = csv_df[\"File name\"].apply(lambda x: x.split('_')[0]) # Add Speaker ID column\n",
    "\n",
    "# Add CER Score for original score\n",
    "for i, row in csv_df.iterrows():\n",
    "    pron_lst = df_assesment[df_assesment['File name'] == row[\"File name\"]].pronScores.values[0].split(' ')\n",
    "    pron_count = pron_lst.count('0')\n",
    "    cer = pron_count/len(pron_lst)\n",
    "    csv_df.loc[i, \"og_cer\"] = cer #df_assesment[\"pronScores\"].apply(lambda x: x.split(' ').count('1')/len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the coulm names in the csv_df\n",
    "csv_df = csv_df.rename(columns={\"File name\": \"file_name\",\"Word\": \"word\", \"CER (Character Error Rate)\": \"transcribed_cer\", \"OG Score\": \"original_score\", \"Transcribed\": \"transcribed\", \"CER Score\": \"transcribed_score\"})\n",
    "\n",
    "# To ulike resultater -> hvor godt matcher CER opp mot OG CER\n",
    "# 2. hvor godt matcher CER/ordet opp mot selve target ordet\n",
    "# csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF9671\"><b> Try: micro-averaging </b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y: [1 5 3 4 2]\n",
      "Value counts in y: original_score\n",
      "5    3179\n",
      "3    2784\n",
      "4    2394\n",
      "2     808\n",
      "1     151\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50        24\n",
      "           2       0.41      0.83      0.55       160\n",
      "           3       0.58      0.67      0.62       543\n",
      "           4       0.00      0.00      0.00       471\n",
      "           5       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.63      1864\n",
      "   macro avg       0.45      0.60      0.51      1864\n",
      "weighted avg       0.48      0.63      0.54      1864\n",
      "\n",
      "Precision (Micro): 0.63\n",
      "Recall (Micro): 0.63\n",
      "F1 Score (Micro): 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['og_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "print(f\"Unique classes in y: {y.unique()}\")\n",
    "print(f\"Value counts in y: {y.value_counts()}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "# clf = DecisionTreeClassifier(random_state=42, class_weight='balanced') \n",
    "clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Micro-averaging metrics\n",
    "\n",
    "# average = 'binary'\n",
    "average = 'micro'\n",
    "# average = 'macro'\n",
    "# average = 'weighted'\n",
    "\n",
    "precision_micro = precision_score(y_test, y_pred, average=average)\n",
    "recall_micro = recall_score(y_test, y_pred, average=average)\n",
    "f1_micro = f1_score(y_test, y_pred, average=average)\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier with SMOTE Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50        24\n",
      "           2       0.41      0.83      0.55       160\n",
      "           3       0.58      0.67      0.62       543\n",
      "           4       0.00      0.00      0.00       471\n",
      "           5       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.63      1864\n",
      "   macro avg       0.45      0.60      0.51      1864\n",
      "weighted avg       0.48      0.63      0.54      1864\n",
      "\n",
      "Precision (Micro): 0.63\n",
      "Recall (Micro): 0.63\n",
      "F1 Score (Micro): 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train with balanced data using RandomForestClassifier\n",
    "clf_rf_smote = RandomForestClassifier(random_state=42)\n",
    "clf_rf_smote.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_rf_smote = clf_rf_smote.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Random Forest Classifier with SMOTE Report\")\n",
    "print(classification_report(y_test, y_pred_rf_smote))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_rf_smote = precision_score(y_test, y_pred_rf_smote, average='micro')\n",
    "recall_micro_rf_smote = recall_score(y_test, y_pred_rf_smote, average='micro')\n",
    "f1_micro_rf_smote = f1_score(y_test, y_pred_rf_smote, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_rf_smote:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_rf_smote:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_rf_smote:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [13:23:40] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.50      0.53        24\n",
      "           1       0.90      0.28      0.43       160\n",
      "           2       0.58      0.97      0.73       543\n",
      "           3       0.00      0.00      0.00       471\n",
      "           4       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.67      1864\n",
      "   macro avg       0.56      0.55      0.51      1864\n",
      "weighted avg       0.52      0.67      0.56      1864\n",
      "\n",
      "Precision (Micro): 0.67\n",
      "Recall (Micro): 0.67\n",
      "F1 Score (Micro): 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['og_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "clf_xgb = xgb.XGBClassifier(random_state=42, scale_pos_weight=1)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_xgb = clf_xgb.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"XGBoost Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_xgb = precision_score(y_test, y_pred_xgb, average='micro')\n",
    "recall_micro_xgb = recall_score(y_test, y_pred_xgb, average='micro')\n",
    "f1_micro_xgb = f1_score(y_test, y_pred_xgb, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_xgb:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_xgb:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50        24\n",
      "           2       0.64      0.46      0.54       160\n",
      "           3       0.59      0.90      0.71       543\n",
      "           4       0.00      0.00      0.00       471\n",
      "           5       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.67      1864\n",
      "   macro avg       0.50      0.57      0.52      1864\n",
      "weighted avg       0.50      0.67      0.57      1864\n",
      "\n",
      "Precision (Micro): 0.67\n",
      "Recall (Micro): 0.67\n",
      "F1 Score (Micro): 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['og_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Define the stacking model\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Stacking Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_stacking = precision_score(y_test, y_pred_stacking, average='micro')\n",
    "recall_micro_stacking = recall_score(y_test, y_pred_stacking, average='micro')\n",
    "f1_micro_stacking = f1_score(y_test, y_pred_stacking, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_stacking:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_stacking:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_stacking:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:92\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_values'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m audio, sample_rate \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(audio_file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Preprocess the audio\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_values\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the Whisper model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:94\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Husker ikke hva dette er for noe\n",
    "# import torch\n",
    "# import soundfile as sf\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # Initialize the WhisperProcessor\n",
    "# processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# # Load an audio file\n",
    "# audio_file = os.path.join(wv_path,name)\n",
    "# audio, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# # Preprocess the audio\n",
    "# input_values = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "# # Load the Whisper model\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# # Generate a prediction\n",
    "# with torch.no_grad():\n",
    "#     logits = model(input_values).logits\n",
    "\n",
    "# # Decode the logits to text\n",
    "# predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "# print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3133737/483509519.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Make the input used for train the classefier model\n",
    "input_assesmen = pd.DataFrame(columns=['CER', 'Score'])\n",
    "\n",
    "same_score = pd.DataFrame(columns=['pronScores', 'Score'])\n",
    "\n",
    "for idx, row in df_assesment.iterrows():\n",
    "    pronScore = row['pronScores'].split(' ')\n",
    "    CER = pronScore.count('0')/len(pronScore)\n",
    "    \n",
    "    one = pronScore.count('1')\n",
    "    values_one = {\n",
    "        'pronScore': one,\n",
    "        'Score': row['Score']\n",
    "    }\n",
    "    values_one = pd.DataFrame(values_one, index=[0])\n",
    "    same_score = pd.concat([same_score, values_one], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    values = {\n",
    "            'CER': CER,\n",
    "            'Score': row['Score']\n",
    "        }\n",
    "    values = pd.DataFrame(values, index=[0])\n",
    "    # if values.empty: No emty enteries\n",
    "    #     print('empty', idx)\n",
    "    # elif input_assesmen.empty:\n",
    "    #     print('empty', idx)\n",
    "        \n",
    "    input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER             [0.]\n",
      "Scores in group [5 4 3]\n",
      "Number of each score in group Score\n",
      "5    3277\n",
      "4    1064\n",
      "3       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.14285714]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    80\n",
      "4    35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.16666667]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    171\n",
      "4     62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.2]\n",
      "Scores in group [4 3]\n",
      "Number of each score in group Score\n",
      "3    341\n",
      "4    224\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.25]\n",
      "Scores in group [3 4 2]\n",
      "Number of each score in group Score\n",
      "3    676\n",
      "4    454\n",
      "2      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.28571429]\n",
      "Scores in group [3 2]\n",
      "Number of each score in group Score\n",
      "3    34\n",
      "2    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.33333333]\n",
      "Scores in group [3 4 2 1 5]\n",
      "Number of each score in group Score\n",
      "3    587\n",
      "4    472\n",
      "2     58\n",
      "1      1\n",
      "5      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.4]\n",
      "Scores in group [2 3 1 4]\n",
      "Number of each score in group Score\n",
      "3    146\n",
      "2     62\n",
      "1      2\n",
      "4      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.42857143]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    16\n",
      "3     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.5]\n",
      "Scores in group [3 2 4 1]\n",
      "Number of each score in group Score\n",
      "3    440\n",
      "2    235\n",
      "4     95\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.57142857]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    10\n",
      "1     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.6]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    70\n",
      "3    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.66666667]\n",
      "Scores in group [3 2 1]\n",
      "Number of each score in group Score\n",
      "3    174\n",
      "2    160\n",
      "1     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.71428571]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    2\n",
      "2    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.75]\n",
      "Scores in group [2 3 1]\n",
      "Number of each score in group Score\n",
      "2    101\n",
      "3     18\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.8]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    14\n",
      "2    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.83333333]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.85714286]\n",
      "Scores in group [1]\n",
      "Number of each score in group Score\n",
      "1    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [1.]\n",
      "Scores in group [3 1 2]\n",
      "Number of each score in group Score\n",
      "1    71\n",
      "2    41\n",
      "3    22\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some of the scores has the same CER value, so this should be filered out before training the model\n",
    "\n",
    "input_group = input_assesmen.groupby('CER')\n",
    "\n",
    "for idx, group in input_group:\n",
    "    print('CER            ', group['CER'].unique())\n",
    "    print('Scores in group', group['Score'].unique())\n",
    "    print('Number of each score in group', group['Score'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fabeb17cd40>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_one[values_one['Score']].CER.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_assesmen)\n",
    "# # NB! some of CER for 3 == CER for 4\n",
    "# # Check how many of the scores has the same CER\n",
    "# print(input_assesmen[input_assesmen['Score'] == 3].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 4].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 5].CER.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.55      0.43        22\n",
      "           2       0.83      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.50      0.55      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#   PROBLEM!!!!!!!!!!!!!!!! IT DOES NOT TRAIN ON NR 4 FOR SOME REASON! WHY?!\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = input_assesmen[['CER']]\n",
    "y = input_assesmen['Score'].astype(int) \n",
    "\n",
    "# Split data\n",
    "# Set random state for reproducibility (same results every time we run the code)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier(random_state=42) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Score: 0.666086060599522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.50      0.40        22\n",
      "           2       0.81      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.49      0.54      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model using GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example prediction for a new data point\n",
    "# new_CER = [[your_new_CER_value]]  # replace with actual new CER value\n",
    "# predicted_score = clf.predict(new_CER)\n",
    "# print(f\"Predicted score for CER {new_CER}: {predicted_score}\")\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SoundFiles_I_want_to_liten_to' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Put specific sound files in a new direcoty:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mSoundFiles_I_want_to_liten_to\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SoundFiles_I_want_to_liten_to' is not defined"
     ]
    }
   ],
   "source": [
    "# Put specific sound files in a new direcoty:\n",
    "\n",
    "new_dir = 'SoundFiles_I_want_to_liten_to'\n",
    "\n",
    "\n",
    "# Kalkuler CER for Assesments og kun bruk cer mot pronScore\n",
    "\n",
    "# df_assesment[df_assesment['File name']=='a01_barn.wav']\n",
    "\n",
    "# total_char = \n",
    "\n",
    "# CER= \n",
    "# TotalÂ NumberÂ ofÂ CharactersÂ inÂ theÂ Reference\n",
    "# NumberÂ ofÂ Insertions+NumberÂ ofÂ Deletions+NumberÂ ofÂ Substitutions\n",
    "# â\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
