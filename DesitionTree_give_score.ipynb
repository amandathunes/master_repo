{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#D65DB1\"> Use Decision Tree Classifier to give score to the transcribed results </span></b>\n",
    "\n",
    "The innput will be the CER score, and the output is the number 1 to 5.\n",
    "The Clasefier is a Decision Tree from sklearn library.\n",
    "\n",
    "Implementation:\n",
    "1)  Get the CER from the assesment file. \n",
    "    This is calculated using pronScores.\n",
    "2)  Make a new df wher the assesment 'CER' and global 'Score' is.\n",
    "3)  Split the df into traing and testing data.\n",
    "4)  Initilize a Decision Tree Classifier\n",
    "5)  Train the classefier, and test it.\n",
    "6)  Opimize the model if needed\n",
    "7)  Change the predicted Score in the transcribed files and update the CSV and results.\n",
    "\n",
    "For 'Score' 5 where 'CER' is more than 0.0, is one incstand at index 7115. With the CER score of 0.333333.\n",
    "\n",
    "<!-- \n",
    "# Code\n",
    "filtered_df = input_assesmen[(input_assesmen['Score'] == 5) & (input_assesmen['CER'] > 0)]\n",
    "print(\"\\nFiltered rows where 'Score' is 5 and 'CER' is greater than 0:\\n\", filtered_df) \n",
    "->\n",
    "\n",
    "<!-- \n",
    "# Test for forstÃ¥else\n",
    "# df_assesment.loc[0]\n",
    "# print(len(df_assesment['Pronunciation'].loc[0].split(' ')))\n",
    "# print(len(df_assesment['pronScores'].loc[0].split(' ')))\n",
    "# pronScore =  df_assesment['pronScores'].loc[0].split(' ')\n",
    "# CER = pronScore.count('0')/len(df_assesment['pronScores'].loc[0].split(' '))\n",
    "\n",
    "# print(CER)\n",
    "# print(pronScore)\n",
    "# print(pronScore.count('0'))\n",
    "# print(pronScore.count('1'))\n",
    "# # Double check that the len of the pronScore is the same as the len of the Pronunciation\n",
    "# for idx, row in df_assesment.iterrows():\n",
    "#     pron = len(row['Pronunciation'].split(' '))\n",
    "#     pronScor = len(row['pronScores'].split(' '))\n",
    "#     if pron != pronScor:\n",
    "#         print('index', idx)\n",
    "#         print('Pronunciation', pron)\n",
    "#         print('pronScores', pronScor) \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors: \n",
    "\n",
    "<b><span style=\"color:#7ABCDD\">7ABCDD</span></b>\n",
    "<b><span style=\"color:#57D0E2\">57D0E2</span></b>\n",
    "<b><span style=\"color:#4DE1D4\">4DE1D4</span></b>\n",
    "<b><span style=\"color:#77EFB6\">77EFB6</span></b>\n",
    "<b><span style=\"color:#B4F790\">B4F790</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#D4A5CA\">D4A5CA</span></b>\n",
    "<b><span style=\"color:#FBA8C2\">FBA8C2</span></b>\n",
    "<b><span style=\"color:#FFAFAC\">FFAFAC</span></b>\n",
    "<b><span style=\"color:#FFC08F\">FFC08F</span></b>\n",
    "<b><span style=\"color:#FFDA77\">FFDA77</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#D65DB1\">D65DB1</span></b>\n",
    "<b><span style=\"color:#FF6F91\">FF6F91</span></b>\n",
    "<b><span style=\"color:#FF9671\">FF9671</span></b>\n",
    "<b><span style=\"color:#FFC75F\">FFC75F</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#2C73D2\">2C73D2</span></b>\n",
    "<b><span style=\"color:#0081CF\">0081CF</span></b>\n",
    "<b><span style=\"color:#0089BA\">0089BA</span></b>\n",
    "<b><span style=\"color:#008E9B\">008E9B</span></b>\n",
    "<b><span style=\"color:#008F7A\">008F7A</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#FF6F91\"> Problems whit the current methode: </span> \n",
    "\n",
    "The dataset is <b>unbalanced</b>. \n",
    "Instances of 'Score' 4 is higest for the same group as 'Score' 5. \n",
    "\n",
    "'Score' 4 is never the highest number for each of the CER scores, so it will not be used in the classefiing process. \n",
    "\n",
    "This means in the reality that <b>the model will only be able to predict 'Score' 1, 2, 3 and 5</b>, where most of them is the lower scores.\n",
    "\n",
    "<span style=\"color:#FF9671\"><b> Try: micro-averaging </b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_made_functions as smf \n",
    "import pandas as pd\n",
    "\n",
    "df_assesment, wv_path = smf.get_correct_df()\n",
    "\n",
    "# # File name\tScore\tProsody\tNoise/Disruption\tPre-speech noise\tRepetition\tWord\tPronunciation\tpronScores\tAssessor\n",
    "csv_df  = pd.read_csv('Transcriptions/transcriptions_nb-whisper-medium-verbatim_v1.csv') # Test for 1 file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>Word</th>\n",
       "      <th>Transcribed</th>\n",
       "      <th>CER (Character Error Rate)</th>\n",
       "      <th>CER Score</th>\n",
       "      <th>OG Score</th>\n",
       "      <th>CER Output</th>\n",
       "      <th>ID</th>\n",
       "      <th>og_cer_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a06_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hyll</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a06</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a12_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a12</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a33_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a33</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d18_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>d18</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a27_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>kylling</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>CharacterOutput(references=[['k', 'y', 'l', 'l...</td>\n",
       "      <td>a27</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>d16_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>snart</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'n', 'a', 'r...</td>\n",
       "      <td>d16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9312</th>\n",
       "      <td>a10_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>schmach</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CharacterOutput(references=[['s', 'c', 'h', 'm...</td>\n",
       "      <td>a10</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>a04_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'm', 'a', 'r...</td>\n",
       "      <td>a04</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>a25_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>snart</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'n', 'a', 'r...</td>\n",
       "      <td>a25</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>a31_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'm', 'a', 'r...</td>\n",
       "      <td>a31</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9316 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          File name   Word Transcribed  CER (Character Error Rate)  CER Score  \\\n",
       "0     a06_hylle.wav  hylle        hyll                    0.250000        NaN   \n",
       "1     a12_hylle.wav  hylle       hylle                    0.000000        NaN   \n",
       "2     a33_hylle.wav  hylle       hylle                    0.000000        NaN   \n",
       "3     d18_hylle.wav  hylle       hylle                    0.000000        NaN   \n",
       "4     a27_hylle.wav  hylle     kylling                    0.571429        NaN   \n",
       "...             ...    ...         ...                         ...        ...   \n",
       "9311  d16_smart.wav  smart       snart                    0.200000        NaN   \n",
       "9312  a10_smart.wav  smart     schmach                    0.571429        NaN   \n",
       "9313  a04_smart.wav  smart       smart                    0.000000        NaN   \n",
       "9314  a25_smart.wav  smart       snart                    0.200000        NaN   \n",
       "9315  a31_smart.wav  smart       smart                    0.000000        NaN   \n",
       "\n",
       "      OG Score                                         CER Output   ID  \\\n",
       "0            1  CharacterOutput(references=[['h', 'y', 'l', 'l...  a06   \n",
       "1            5  CharacterOutput(references=[['h', 'y', 'l', 'l...  a12   \n",
       "2            5  CharacterOutput(references=[['h', 'y', 'l', 'l...  a33   \n",
       "3            5  CharacterOutput(references=[['h', 'y', 'l', 'l...  d18   \n",
       "4            3  CharacterOutput(references=[['k', 'y', 'l', 'l...  a27   \n",
       "...        ...                                                ...  ...   \n",
       "9311         4  CharacterOutput(references=[['s', 'n', 'a', 'r...  d16   \n",
       "9312         2  CharacterOutput(references=[['s', 'c', 'h', 'm...  a10   \n",
       "9313         4  CharacterOutput(references=[['s', 'm', 'a', 'r...  a04   \n",
       "9314         4  CharacterOutput(references=[['s', 'n', 'a', 'r...  a25   \n",
       "9315         4  CharacterOutput(references=[['s', 'm', 'a', 'r...  a31   \n",
       "\n",
       "      og_cer_score  \n",
       "0         0.800000  \n",
       "1         0.666667  \n",
       "2         0.666667  \n",
       "3         0.500000  \n",
       "4         0.666667  \n",
       "...            ...  \n",
       "9311      1.000000  \n",
       "9312      0.666667  \n",
       "9313      1.000000  \n",
       "9314      0.800000  \n",
       "9315      0.500000  \n",
       "\n",
       "[9316 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make al values in cloumn = NaN\n",
    "csv_df[\"CER Score\"] = float('nan')\n",
    "# Add ID column\n",
    "csv_df[\"ID\"] = csv_df[\"File name\"].apply(lambda x: x.split('_')[0])\n",
    "# Add CER Score for original score\n",
    "csv_df[\"og_cer_score\"] = df_assesment[\"pronScores\"].apply(lambda x: x.split(' ').count('1')/len(x.split(' ')))\n",
    "\n",
    "# To ulike resultater -> hvor godt matcher CER opp mot OG CER\n",
    "# 2. hvor godt matcher CER/ordet opp mot selve target ordet\n",
    "\n",
    "csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_values'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# audio, sample_rate = sf.read(audio_file)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# # Preprocess the audio\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_values\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# # Generate a prediction\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_name_path = 'NbAiLabBeta/nb-whisper-base'\n",
    "model = pipeline(\"automatic-speech-recognition\", model_name_path) # Load the model\n",
    "\n",
    "# From Phind.com\n",
    "from transformers import WhisperTokenizer, WhisperForConditionalGeneration\n",
    "import soundfile as sf\n",
    "import os\n",
    "name = 'd17_loeve.wav'\n",
    "# Load an audio file\n",
    "audio_file = os.path.join(wv_path,name)\n",
    "\n",
    "transcription = model(audio_file, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "\n",
    "# Load the Whisper model and tokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_path)\n",
    "\n",
    "# audio, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# # Preprocess the audio\n",
    "input_values = tokenizer(transcription['text'], return_tensors=\"pt\").input_values\n",
    "\n",
    "# # Generate a prediction\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# # Decode the logits to text\n",
    "# predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# transcription = tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "# print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:92\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_values'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m audio, sample_rate \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(audio_file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Preprocess the audio\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_values\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the Whisper model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:94\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Initialize the WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Load an audio file\n",
    "audio_file = os.path.join(wv_path,name)\n",
    "audio, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# Preprocess the audio\n",
    "input_values = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "# Load the Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Generate a prediction\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Decode the logits to text\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3133737/483509519.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Make the input used for train the classefier model\n",
    "input_assesmen = pd.DataFrame(columns=['CER', 'Score'])\n",
    "\n",
    "same_score = pd.DataFrame(columns=['pronScores', 'Score'])\n",
    "\n",
    "for idx, row in df_assesment.iterrows():\n",
    "    pronScore = row['pronScores'].split(' ')\n",
    "    CER = pronScore.count('0')/len(pronScore)\n",
    "    \n",
    "    one = pronScore.count('1')\n",
    "    values_one = {\n",
    "        'pronScore': one,\n",
    "        'Score': row['Score']\n",
    "    }\n",
    "    values_one = pd.DataFrame(values_one, index=[0])\n",
    "    same_score = pd.concat([same_score, values_one], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    values = {\n",
    "            'CER': CER,\n",
    "            'Score': row['Score']\n",
    "        }\n",
    "    values = pd.DataFrame(values, index=[0])\n",
    "    # if values.empty: No emty enteries\n",
    "    #     print('empty', idx)\n",
    "    # elif input_assesmen.empty:\n",
    "    #     print('empty', idx)\n",
    "        \n",
    "    input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER             [0.]\n",
      "Scores in group [5 4 3]\n",
      "Number of each score in group Score\n",
      "5    3277\n",
      "4    1064\n",
      "3       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.14285714]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    80\n",
      "4    35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.16666667]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    171\n",
      "4     62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.2]\n",
      "Scores in group [4 3]\n",
      "Number of each score in group Score\n",
      "3    341\n",
      "4    224\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.25]\n",
      "Scores in group [3 4 2]\n",
      "Number of each score in group Score\n",
      "3    676\n",
      "4    454\n",
      "2      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.28571429]\n",
      "Scores in group [3 2]\n",
      "Number of each score in group Score\n",
      "3    34\n",
      "2    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.33333333]\n",
      "Scores in group [3 4 2 1 5]\n",
      "Number of each score in group Score\n",
      "3    587\n",
      "4    472\n",
      "2     58\n",
      "1      1\n",
      "5      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.4]\n",
      "Scores in group [2 3 1 4]\n",
      "Number of each score in group Score\n",
      "3    146\n",
      "2     62\n",
      "1      2\n",
      "4      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.42857143]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    16\n",
      "3     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.5]\n",
      "Scores in group [3 2 4 1]\n",
      "Number of each score in group Score\n",
      "3    440\n",
      "2    235\n",
      "4     95\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.57142857]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    10\n",
      "1     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.6]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    70\n",
      "3    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.66666667]\n",
      "Scores in group [3 2 1]\n",
      "Number of each score in group Score\n",
      "3    174\n",
      "2    160\n",
      "1     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.71428571]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    2\n",
      "2    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.75]\n",
      "Scores in group [2 3 1]\n",
      "Number of each score in group Score\n",
      "2    101\n",
      "3     18\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.8]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    14\n",
      "2    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.83333333]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.85714286]\n",
      "Scores in group [1]\n",
      "Number of each score in group Score\n",
      "1    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [1.]\n",
      "Scores in group [3 1 2]\n",
      "Number of each score in group Score\n",
      "1    71\n",
      "2    41\n",
      "3    22\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some of the scores has the same CER value, so this should be filered out before training the model\n",
    "\n",
    "input_group = input_assesmen.groupby('CER')\n",
    "\n",
    "for idx, group in input_group:\n",
    "    print('CER            ', group['CER'].unique())\n",
    "    print('Scores in group', group['Score'].unique())\n",
    "    print('Number of each score in group', group['Score'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fabeb17cd40>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_one[values_one['Score']].CER.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_assesmen)\n",
    "# # NB! some of CER for 3 == CER for 4\n",
    "# # Check how many of the scores has the same CER\n",
    "# print(input_assesmen[input_assesmen['Score'] == 3].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 4].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 5].CER.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.55      0.43        22\n",
      "           2       0.83      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.50      0.55      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#   PROBLEM!!!!!!!!!!!!!!!! IT DOES NOT TRAIN ON NR 4 FOR SOME REASON! WHY?!\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = input_assesmen[['CER']]\n",
    "y = input_assesmen['Score'].astype(int) \n",
    "\n",
    "# Split data\n",
    "# Set random state for reproducibility (same results every time we run the code)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier(random_state=42) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Score: 0.666086060599522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.50      0.40        22\n",
      "           2       0.81      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.49      0.54      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model using GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example prediction for a new data point\n",
    "# new_CER = [[your_new_CER_value]]  # replace with actual new CER value\n",
    "# predicted_score = clf.predict(new_CER)\n",
    "# print(f\"Predicted score for CER {new_CER}: {predicted_score}\")\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SoundFiles_I_want_to_liten_to' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Put specific sound files in a new direcoty:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mSoundFiles_I_want_to_liten_to\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SoundFiles_I_want_to_liten_to' is not defined"
     ]
    }
   ],
   "source": [
    "# Put specific sound files in a new direcoty:\n",
    "\n",
    "new_dir = 'SoundFiles_I_want_to_liten_to'\n",
    "\n",
    "\n",
    "# Kalkuler CER for Assesments og kun bruk cer mot pronScore\n",
    "\n",
    "# df_assesment[df_assesment['File name']=='a01_barn.wav']\n",
    "\n",
    "# total_char = \n",
    "\n",
    "# CER= \n",
    "# TotalÂ NumberÂ ofÂ CharactersÂ inÂ theÂ Reference\n",
    "# NumberÂ ofÂ Insertions+NumberÂ ofÂ Deletions+NumberÂ ofÂ Substitutions\n",
    "# â\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
