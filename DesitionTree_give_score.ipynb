{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#D65DB1\"> Use Decision Tree Classifier to give score to the transcribed results </span></b>\n",
    "\n",
    "The innput will be the CER score, and the output is the number 1 to 5.\n",
    "The Clasefier is a Decision Tree from sklearn library.\n",
    "\n",
    "Implementation:\n",
    "1)  Get the CER from the assesment file. \n",
    "    This is calculated using pronScores.\n",
    "2)  Make a new df wher the assesment 'CER' and global 'Score' is.\n",
    "3)  Split the df into traing and testing data.\n",
    "4)  Initilize a Decision Tree Classifier\n",
    "5)  Train the classefier, and test it.\n",
    "6)  Opimize the model if needed\n",
    "7)  Change the predicted Score in the transcribed files and update the CSV and results.\n",
    "\n",
    "For 'Score' 5 where 'CER' is more than 0.0, is one incstand at index 7115. With the CER score of 0.333333.\n",
    "\n",
    "<!-- \n",
    "# Code\n",
    "filtered_df = input_assesmen[(input_assesmen['Score'] == 5) & (input_assesmen['CER'] > 0)]\n",
    "print(\"\\nFiltered rows where 'Score' is 5 and 'CER' is greater than 0:\\n\", filtered_df) \n",
    "->\n",
    "\n",
    "<!-- \n",
    "# Test for forstÃ¥else\n",
    "# df_assesment.loc[0]\n",
    "# print(len(df_assesment['Pronunciation'].loc[0].split(' ')))\n",
    "# print(len(df_assesment['pronScores'].loc[0].split(' ')))\n",
    "# pronScore =  df_assesment['pronScores'].loc[0].split(' ')\n",
    "# CER = pronScore.count('0')/len(df_assesment['pronScores'].loc[0].split(' '))\n",
    "\n",
    "# print(CER)\n",
    "# print(pronScore)\n",
    "# print(pronScore.count('0'))\n",
    "# print(pronScore.count('1'))\n",
    "# # Double check that the len of the pronScore is the same as the len of the Pronunciation\n",
    "# for idx, row in df_assesment.iterrows():\n",
    "#     pron = len(row['Pronunciation'].split(' '))\n",
    "#     pronScor = len(row['pronScores'].split(' '))\n",
    "#     if pron != pronScor:\n",
    "#         print('index', idx)\n",
    "#         print('Pronunciation', pron)\n",
    "#         print('pronScores', pronScor) \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors: \n",
    "\n",
    "<b><span style=\"color:#7ABCDD\">7ABCDD</span></b>\n",
    "<b><span style=\"color:#57D0E2\">57D0E2</span></b>\n",
    "<b><span style=\"color:#4DE1D4\">4DE1D4</span></b>\n",
    "<b><span style=\"color:#77EFB6\">77EFB6</span></b>\n",
    "<b><span style=\"color:#B4F790\">B4F790</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#D4A5CA\">D4A5CA</span></b>\n",
    "<b><span style=\"color:#FBA8C2\">FBA8C2</span></b>\n",
    "<b><span style=\"color:#FFAFAC\">FFAFAC</span></b>\n",
    "<b><span style=\"color:#FFC08F\">FFC08F</span></b>\n",
    "<b><span style=\"color:#FFDA77\">FFDA77</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#D65DB1\">D65DB1</span></b>\n",
    "<b><span style=\"color:#FF6F91\">FF6F91</span></b>\n",
    "<b><span style=\"color:#FF9671\">FF9671</span></b>\n",
    "<b><span style=\"color:#FFC75F\">FFC75F</span></b>\n",
    "<b><span style=\"color:#F9F871\">F9F871</span></b>\n",
    "\n",
    "<b><span style=\"color:#845EC2\">845EC2</span></b>\n",
    "<b><span style=\"color:#2C73D2\">2C73D2</span></b>\n",
    "<b><span style=\"color:#0081CF\">0081CF</span></b>\n",
    "<b><span style=\"color:#0089BA\">0089BA</span></b>\n",
    "<b><span style=\"color:#008E9B\">008E9B</span></b>\n",
    "<b><span style=\"color:#008F7A\">008F7A</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#FF6F91\"> Problems whit the current methode: </span> \n",
    "\n",
    "The dataset is <b>unbalanced</b>. \n",
    "Instances of 'Score' 4 is higest for the same group as 'Score' 5. \n",
    "\n",
    "'Score' 4 is never the highest number for each of the CER scores, so it will not be used in the classefiing process. \n",
    "\n",
    "This means in the reality that <b>the model will only be able to predict 'Score' 1, 2, 3 and 5</b>, where most of them is the lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import self_made_functions as smf \n",
    "import pandas as pd\n",
    "\n",
    "df_assesment, wv_path = smf.get_correct_df()\n",
    "\n",
    "# # File name\tScore\tProsody\tNoise/Disruption\tPre-speech noise\tRepetition\tWord\tPronunciation\tpronScores\tAssessor\n",
    "csv_df  = pd.read_csv('Transcriptions/transcriptions_nb-whisper-medium-verbatim_v1.csv') # Test for 1 file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = df_assesment[df_assesment['File name'] == 'a12_hylle.wav']\n",
    "# pron_lst = row.pronScores.values[0].split(' ')\n",
    "# pron_count = pron_lst.count('0')\n",
    "# cer = pron_count/len(pron_lst)\n",
    "# cer\n",
    "# print(pron_lst, pron_count, len(pron_lst), cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"color:#FFC75F\"> Add ekstra information columns </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df[\"CER Score\"] = float('nan') # Remove the scores since they are not correct calculated\n",
    "csv_df[\"ID\"] = csv_df[\"File name\"].apply(lambda x: x.split('_')[0]) # Add Speaker ID column\n",
    "\n",
    "# Add CER Score for original score\n",
    "for i, row in csv_df.iterrows():\n",
    "    pron_lst = df_assesment[df_assesment['File name'] == row[\"File name\"]].pronScores.values[0].split(' ')\n",
    "    pron_count = pron_lst.count('0')\n",
    "    cer = pron_count/len(pron_lst)\n",
    "    csv_df.loc[i, \"og_cer\"] = cer #df_assesment[\"pronScores\"].apply(lambda x: x.split(' ').count('1')/len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the coulm names in the csv_df\n",
    "csv_df = csv_df.rename(columns={\"File name\": \"file_name\",\n",
    "                                \"Word\": \"word\", \n",
    "                                \"CER (Character Error Rate)\": \"transcribed_cer\", \"OG Score\": \"original_score\", \n",
    "                                \"Transcribed\": \"transcribed\", \"CER Score\": \"transcribed_score\"})\n",
    "\n",
    "# To ulike resultater -> hvor godt matcher CER opp mot OG CER\n",
    "# 2. hvor godt matcher CER/ordet opp mot selve target ordet\n",
    "# csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF9671\"><b> Try: micro-averaging </b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>word</th>\n",
       "      <th>transcribed</th>\n",
       "      <th>transcribed_cer</th>\n",
       "      <th>transcribed_score</th>\n",
       "      <th>original_score</th>\n",
       "      <th>CER Output</th>\n",
       "      <th>ID</th>\n",
       "      <th>og_cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a06_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hyll</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a06</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a12_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a33_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>a33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d18_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>hylle</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>CharacterOutput(references=[['h', 'y', 'l', 'l...</td>\n",
       "      <td>d18</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a27_hylle.wav</td>\n",
       "      <td>hylle</td>\n",
       "      <td>kylling</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>CharacterOutput(references=[['k', 'y', 'l', 'l...</td>\n",
       "      <td>a27</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>d16_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>snart</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'n', 'a', 'r...</td>\n",
       "      <td>d16</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9312</th>\n",
       "      <td>a10_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>schmach</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CharacterOutput(references=[['s', 'c', 'h', 'm...</td>\n",
       "      <td>a10</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>a04_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'm', 'a', 'r...</td>\n",
       "      <td>a04</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>a25_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>snart</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'n', 'a', 'r...</td>\n",
       "      <td>a25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>a31_smart.wav</td>\n",
       "      <td>smart</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>CharacterOutput(references=[['s', 'm', 'a', 'r...</td>\n",
       "      <td>a31</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9316 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name   word transcribed  transcribed_cer  transcribed_score  \\\n",
       "0     a06_hylle.wav  hylle        hyll         0.250000                NaN   \n",
       "1     a12_hylle.wav  hylle       hylle         0.000000                NaN   \n",
       "2     a33_hylle.wav  hylle       hylle         0.000000                NaN   \n",
       "3     d18_hylle.wav  hylle       hylle         0.000000                NaN   \n",
       "4     a27_hylle.wav  hylle     kylling         0.571429                NaN   \n",
       "...             ...    ...         ...              ...                ...   \n",
       "9311  d16_smart.wav  smart       snart         0.200000                NaN   \n",
       "9312  a10_smart.wav  smart     schmach         0.571429                NaN   \n",
       "9313  a04_smart.wav  smart       smart         0.000000                NaN   \n",
       "9314  a25_smart.wav  smart       snart         0.200000                NaN   \n",
       "9315  a31_smart.wav  smart       smart         0.000000                NaN   \n",
       "\n",
       "      original_score                                         CER Output   ID  \\\n",
       "0                  1  CharacterOutput(references=[['h', 'y', 'l', 'l...  a06   \n",
       "1                  5  CharacterOutput(references=[['h', 'y', 'l', 'l...  a12   \n",
       "2                  5  CharacterOutput(references=[['h', 'y', 'l', 'l...  a33   \n",
       "3                  5  CharacterOutput(references=[['h', 'y', 'l', 'l...  d18   \n",
       "4                  3  CharacterOutput(references=[['k', 'y', 'l', 'l...  a27   \n",
       "...              ...                                                ...  ...   \n",
       "9311               4  CharacterOutput(references=[['s', 'n', 'a', 'r...  d16   \n",
       "9312               2  CharacterOutput(references=[['s', 'c', 'h', 'm...  a10   \n",
       "9313               4  CharacterOutput(references=[['s', 'm', 'a', 'r...  a04   \n",
       "9314               4  CharacterOutput(references=[['s', 'n', 'a', 'r...  a25   \n",
       "9315               4  CharacterOutput(references=[['s', 'm', 'a', 'r...  a31   \n",
       "\n",
       "      og_cer  \n",
       "0       0.50  \n",
       "1       0.00  \n",
       "2       0.00  \n",
       "3       0.00  \n",
       "4       0.25  \n",
       "...      ...  \n",
       "9311    0.25  \n",
       "9312    0.50  \n",
       "9313    0.25  \n",
       "9314    0.25  \n",
       "9315    0.25  \n",
       "\n",
       "[9316 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y: [1 5 3 4 2]\n",
      "Value counts in y: original_score\n",
      "5    3179\n",
      "3    2784\n",
      "4    2394\n",
      "2     808\n",
      "1     151\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.06      0.58      0.11        24\n",
      "           2       0.15      0.36      0.22       160\n",
      "           3       0.42      0.29      0.34       543\n",
      "           4       0.14      0.00      0.00       471\n",
      "           5       0.54      0.71      0.61       666\n",
      "\n",
      "    accuracy                           0.38      1864\n",
      "   macro avg       0.26      0.39      0.26      1864\n",
      "weighted avg       0.37      0.38      0.34      1864\n",
      "\n",
      "Precision (Micro): 0.38\n",
      "Recall (Micro): 0.38\n",
      "F1 Score (Micro): 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['transcribed_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "print(f\"Unique classes in y: {y.unique()}\")\n",
    "print(f\"Value counts in y: {y.value_counts()}\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "# clf = DecisionTreeClassifier(random_state=42, class_weight='balanced') \n",
    "clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Micro-averaging metrics\n",
    "\n",
    "# average = 'binary'\n",
    "average = 'micro'\n",
    "# average = 'macro'\n",
    "# average = 'weighted'\n",
    "\n",
    "precision_micro = precision_score(y_test, y_pred, average=average)\n",
    "recall_micro = recall_score(y_test, y_pred, average=average)\n",
    "f1_micro = f1_score(y_test, y_pred, average=average)\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier with SMOTE Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50        24\n",
      "           2       0.41      0.83      0.55       160\n",
      "           3       0.58      0.67      0.62       543\n",
      "           4       0.00      0.00      0.00       471\n",
      "           5       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.63      1864\n",
      "   macro avg       0.45      0.60      0.51      1864\n",
      "weighted avg       0.48      0.63      0.54      1864\n",
      "\n",
      "Precision (Micro): 0.63\n",
      "Recall (Micro): 0.63\n",
      "F1 Score (Micro): 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train with balanced data using RandomForestClassifier\n",
    "clf_rf_smote = RandomForestClassifier(random_state=42)\n",
    "clf_rf_smote.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_rf_smote = clf_rf_smote.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Random Forest Classifier with SMOTE Report\")\n",
    "print(classification_report(y_test, y_pred_rf_smote))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_rf_smote = precision_score(y_test, y_pred_rf_smote, average='micro')\n",
    "recall_micro_rf_smote = recall_score(y_test, y_pred_rf_smote, average='micro')\n",
    "f1_micro_rf_smote = f1_score(y_test, y_pred_rf_smote, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_rf_smote:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_rf_smote:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_rf_smote:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [08:43:16] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.50      0.53        24\n",
      "           1       0.90      0.28      0.43       160\n",
      "           2       0.58      0.97      0.73       543\n",
      "           3       0.00      0.00      0.00       471\n",
      "           4       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.67      1864\n",
      "   macro avg       0.56      0.55      0.51      1864\n",
      "weighted avg       0.52      0.67      0.56      1864\n",
      "\n",
      "Precision (Micro): 0.67\n",
      "Recall (Micro): 0.67\n",
      "F1 Score (Micro): 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['og_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "clf_xgb = xgb.XGBClassifier(random_state=42, scale_pos_weight=1)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_xgb = clf_xgb.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"XGBoost Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_xgb = precision_score(y_test, y_pred_xgb, average='micro')\n",
    "recall_micro_xgb = recall_score(y_test, y_pred_xgb, average='micro')\n",
    "f1_micro_xgb = f1_score(y_test, y_pred_xgb, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_xgb:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_xgb:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50        24\n",
      "           2       0.64      0.46      0.54       160\n",
      "           3       0.59      0.90      0.71       543\n",
      "           4       0.00      0.00      0.00       471\n",
      "           5       0.75      1.00      0.86       666\n",
      "\n",
      "    accuracy                           0.67      1864\n",
      "   macro avg       0.50      0.57      0.52      1864\n",
      "weighted avg       0.50      0.67      0.57      1864\n",
      "\n",
      "Precision (Micro): 0.67\n",
      "Recall (Micro): 0.67\n",
      "F1 Score (Micro): 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = csv_df[['og_cer']]\n",
    "y = csv_df['original_score'].astype(int) \n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Define the stacking model\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), n_jobs=-1)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Stacking Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n",
    "\n",
    "# Micro-averaging metrics\n",
    "precision_micro_stacking = precision_score(y_test, y_pred_stacking, average='micro')\n",
    "recall_micro_stacking = recall_score(y_test, y_pred_stacking, average='micro')\n",
    "f1_micro_stacking = f1_score(y_test, y_pred_stacking, average='micro')\n",
    "\n",
    "print(f\"Precision (Micro): {precision_micro_stacking:.2f}\")\n",
    "print(f\"Recall (Micro): {recall_micro_stacking:.2f}\")\n",
    "print(f\"F1 Score (Micro): {f1_micro_stacking:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Husker ikke hva dette er for noe\n",
    "# import torch\n",
    "# import soundfile as sf\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # Initialize the WhisperProcessor\n",
    "# processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# # Load an audio file\n",
    "# audio_file = os.path.join(wv_path,name)\n",
    "# audio, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# # Preprocess the audio\n",
    "# input_values = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "# # Load the Whisper model\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# # Generate a prediction\n",
    "# with torch.no_grad():\n",
    "#     logits = model(input_values).logits\n",
    "\n",
    "# # Decode the logits to text\n",
    "# predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "# print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1514401/483509519.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Make the input used for train the classefier model\n",
    "input_assesmen = pd.DataFrame(columns=['CER', 'Score'])\n",
    "\n",
    "same_score = pd.DataFrame(columns=['pronScores', 'Score'])\n",
    "\n",
    "for idx, row in df_assesment.iterrows():\n",
    "    pronScore = row['pronScores'].split(' ')\n",
    "    CER = pronScore.count('0')/len(pronScore)\n",
    "    \n",
    "    one = pronScore.count('1')\n",
    "    values_one = {\n",
    "        'pronScore': one,\n",
    "        'Score': row['Score']\n",
    "    }\n",
    "    values_one = pd.DataFrame(values_one, index=[0])\n",
    "    same_score = pd.concat([same_score, values_one], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    values = {\n",
    "            'CER': CER,\n",
    "            'Score': row['Score']\n",
    "        }\n",
    "    values = pd.DataFrame(values, index=[0])\n",
    "    # if values.empty: No emty enteries\n",
    "    #     print('empty', idx)\n",
    "    # elif input_assesmen.empty:\n",
    "    #     print('empty', idx)\n",
    "        \n",
    "    input_assesmen = pd.concat([input_assesmen, values], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER             [0.]\n",
      "Scores in group [5 4 3]\n",
      "Number of each score in group Score\n",
      "5    3191\n",
      "4    1146\n",
      "3       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.14285714]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    81\n",
      "4    34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.16666667]\n",
      "Scores in group [3 4]\n",
      "Number of each score in group Score\n",
      "3    171\n",
      "4     62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.2]\n",
      "Scores in group [4 3]\n",
      "Number of each score in group Score\n",
      "3    341\n",
      "4    224\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.25]\n",
      "Scores in group [3 4 2]\n",
      "Number of each score in group Score\n",
      "3    676\n",
      "4    453\n",
      "2      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.28571429]\n",
      "Scores in group [3 2]\n",
      "Number of each score in group Score\n",
      "3    34\n",
      "2    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.33333333]\n",
      "Scores in group [3 4 2 1 5]\n",
      "Number of each score in group Score\n",
      "3    586\n",
      "4    472\n",
      "2     59\n",
      "1      1\n",
      "5      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.4]\n",
      "Scores in group [2 3 1 4]\n",
      "Number of each score in group Score\n",
      "3    146\n",
      "2     62\n",
      "1      2\n",
      "4      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.42857143]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    16\n",
      "3     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.5]\n",
      "Scores in group [3 2 4 1]\n",
      "Number of each score in group Score\n",
      "3    439\n",
      "2    236\n",
      "4     95\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.57142857]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    10\n",
      "1     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.6]\n",
      "Scores in group [2 3]\n",
      "Number of each score in group Score\n",
      "2    70\n",
      "3    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.66666667]\n",
      "Scores in group [3 2 1]\n",
      "Number of each score in group Score\n",
      "3    174\n",
      "2    160\n",
      "1     15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.71428571]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    2\n",
      "2    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.75]\n",
      "Scores in group [2 3 1]\n",
      "Number of each score in group Score\n",
      "2    101\n",
      "3     18\n",
      "1     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.8]\n",
      "Scores in group [1 2]\n",
      "Number of each score in group Score\n",
      "1    14\n",
      "2    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.83333333]\n",
      "Scores in group [2 1]\n",
      "Number of each score in group Score\n",
      "2    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [0.85714286]\n",
      "Scores in group [1]\n",
      "Number of each score in group Score\n",
      "1    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CER             [1.]\n",
      "Scores in group [3 1 2]\n",
      "Number of each score in group Score\n",
      "1    71\n",
      "2    41\n",
      "3    22\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some of the scores has the same CER value, so this should be filered out before training the model\n",
    "\n",
    "input_group = input_assesmen.groupby('CER')\n",
    "\n",
    "for idx, group in input_group:\n",
    "    print('CER            ', group['CER'].unique())\n",
    "    print('Scores in group', group['Score'].unique())\n",
    "    print('Number of each score in group', group['Score'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([4], dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalues_one\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalues_one\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mCER\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master_amanda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([4], dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "values_one[values_one['Score']].CER.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_assesmen)\n",
    "# # NB! some of CER for 3 == CER for 4\n",
    "# # Check how many of the scores has the same CER\n",
    "# print(input_assesmen[input_assesmen['Score'] == 3].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 4].CER.value_counts())\n",
    "# print(input_assesmen[input_assesmen['Score'] == 5].CER.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.55      0.43        22\n",
      "           2       0.83      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.50      0.55      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#   PROBLEM!!!!!!!!!!!!!!!! IT DOES NOT TRAIN ON NR 4 FOR SOME REASON! WHY?!\n",
    "\n",
    "# Split data into X (features) and y (labels)\n",
    "X = input_assesmen[['CER']]\n",
    "y = input_assesmen['Score'].astype(int) \n",
    "\n",
    "# Split data\n",
    "# Set random state for reproducibility (same results every time we run the code)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "clf = DecisionTreeClassifier(random_state=42) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best Score: 0.666086060599522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.50      0.40        22\n",
      "           2       0.81      0.23      0.35       173\n",
      "           3       0.58      0.98      0.73       550\n",
      "           4       0.00      0.00      0.00       487\n",
      "           5       0.75      1.00      0.85       633\n",
      "\n",
      "    accuracy                           0.66      1865\n",
      "   macro avg       0.49      0.54      0.47      1865\n",
      "weighted avg       0.50      0.66      0.54      1865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ajtruyen/anaconda3/envs/master_amanda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model using GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example prediction for a new data point\n",
    "# new_CER = [[your_new_CER_value]]  # replace with actual new CER value\n",
    "# predicted_score = clf.predict(new_CER)\n",
    "# print(f\"Predicted score for CER {new_CER}: {predicted_score}\")\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SoundFiles_I_want_to_liten_to' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Put specific sound files in a new direcoty:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mSoundFiles_I_want_to_liten_to\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SoundFiles_I_want_to_liten_to' is not defined"
     ]
    }
   ],
   "source": [
    "# Put specific sound files in a new direcoty:\n",
    "\n",
    "new_dir = 'SoundFiles_I_want_to_liten_to'\n",
    "\n",
    "\n",
    "# Kalkuler CER for Assesments og kun bruk cer mot pronScore\n",
    "\n",
    "# df_assesment[df_assesment['File name']=='a01_barn.wav']\n",
    "\n",
    "# total_char = \n",
    "\n",
    "# CER= \n",
    "# TotalÂ NumberÂ ofÂ CharactersÂ inÂ theÂ Reference\n",
    "# NumberÂ ofÂ Insertions+NumberÂ ofÂ Deletions+NumberÂ ofÂ Substitutions\n",
    "# â\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
