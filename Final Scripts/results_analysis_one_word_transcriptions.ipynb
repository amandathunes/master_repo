{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#845EC2\">**Results and Analysis :** One word transcriptions </span> \n",
    "\n",
    "Focus on <b><span style=\"color:#D65DB1\"> version 1 </span></b> for the transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><span style=\"color:#FF6F91\"> Library Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 15:09:56.620871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 15:09:57.587625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import sys      \n",
    "script_directory = '../'\n",
    "sys.path.append(script_directory)\n",
    "import self_made_functions as smf\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from   textwrap import wrap \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jiwer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><span style=\"color:#FF9671\"> Data initialization  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assessment, wv_path = smf.get_correct_df()\n",
    "\n",
    "# Make a new directory for resaved files \n",
    "results_dir = '../Transcriptions/Results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"Directory {results_dir} created\")\n",
    "\n",
    "# Read empty transcriptions\n",
    "empty_transcriptions = pd.read_csv('../Transcriptions/true_empty_transcriptions_v1.csv')\n",
    "\n",
    "# Transcription directory information\n",
    "transcription_dir = '../Transcriptions'\n",
    "lst_csv = os.listdir(transcription_dir)\n",
    "# lst_csv = [file for file in lst_csv if file.startswith('tran') & file.endswith('v1.csv')] # The v1 transcribed files\n",
    "lst_csv = [file for file in lst_csv if file.startswith('true_tran') & file.endswith('v1.csv')] # The v1 transcribed files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list by model name\n",
    "test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "                    'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "                    'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "lst_csv = sorted(lst_csv, key=lambda x: test_for_models.index(x.split('_')[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b><span style=\"color:#FF6F91\"> Save missing information </span></b>\n",
    "\n",
    " The transcription files are changed and re saved.\n",
    "\n",
    "The <b><span style=\"color:#ff9671\">empty-transcriptions</span></b> are added to the transcriptions.\n",
    "\n",
    "The <b><span style=\"color:#ff9671\">Phonetic Error Ratio (PER)</span></b> is calculated. \n",
    "\n",
    "A <b><span style=\"color:#ff9671\">ID column</span></b> is added, and all the colum names are <b><span style=\"color:#ff9671\">renamed</span></b> and organized.\n",
    "\n",
    "\n",
    "A column for the <b><span style=\"color:#ff9671\">model name</span></b> is also added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resave_transcription_df(bool:True):\n",
    "    lst_path = [os.path.join(transcription_dir, file) for file in lst_csv]\n",
    "\n",
    "    # Iterate through the CSV files to add what is missing\n",
    "    for path in lst_path:    \n",
    "        # Read transcriped files\n",
    "        df_csv = pd.read_csv(path)\n",
    "        print(f\"Reading {path}\")\n",
    "\n",
    "        # For old transcriptions ------\n",
    "        # df_csv = df_csv.drop(['CER Output',\"CER Score\"], axis=1) # axis = 1 : drops column, = 0 : drops row \n",
    "        # # Fix the CER column - For comparison\n",
    "        # df_csv['CER (Character Error Rate)'] = df_csv.apply(lambda x: jiwer.cer(x['Word'], x['Transcribed']), axis=1)\n",
    "        # ------------------------------\n",
    "        \n",
    "        # Add a column with the striped transcription and its corresponding CER.\n",
    "        # This is interning regarding the performance of the verbatim model.\n",
    "        # All transcriptions, excluding the empty transcriptions, are striped for all models, including verbatim.\n",
    "        df_csv['transcribed_word_striped'] = df_csv['Transcribed'].apply(\n",
    "            lambda x: x.strip().lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\") \n",
    "            if isinstance(x, str) else x)      \n",
    "        df_csv['striped_CER'] = df_csv.apply(lambda x: jiwer.cer(x['Word'], x['transcribed_word_striped']), axis=1)\n",
    "        # print(df_csv[df_csv['File name']=='a01_sykkel.wav'])\n",
    "        \n",
    "        # Add model name\n",
    "        model_name = path.split('_')[-2]\n",
    "        df_csv[\"model_name\"] = model_name\n",
    "        \n",
    "        # # Change names - for old transcriptions ------\n",
    "        # df_csv = df_csv.rename(columns={\"File name\": \"file_name\",\n",
    "        #                             \"CER (Character Error Rate)\": \"CER\", # Character Error Rate (CER)\n",
    "        #                             \"Word\": \"target_word\", \n",
    "        #                             \"Transcribed\": \"transcribed_word\", \n",
    "        #                             \"OG Score\": \"global_score\"})\n",
    "        # -----------------------------------------------\n",
    "        df_csv = df_csv.rename(columns={\"File name\": \"file_name\",\n",
    "                                    \"CER (Character Error Rate)\": \"CER\", # Character Error Rate (CER)\n",
    "                                    \"Word\": \"target_word\", \n",
    "                                    \"Transcribed\": \"transcribed_word\", \n",
    "                                    \"CER Score\": \"global_score\"})\n",
    "        \n",
    "        for i, row in df_csv.iterrows():\n",
    "            pron_lst = df_assessment[df_assessment['File name'] == row[\"file_name\"]].pronScores.values[0].split(' ')\n",
    "            pron_count = pron_lst.count('0')\n",
    "            per = pron_count/len(pron_lst)\n",
    "            df_csv.loc[i, \"PER\"] = per # Phonetic Error Rate (PER)\n",
    "        \n",
    "        df_csv = pd.concat([df_csv, empty_transcriptions[empty_transcriptions['model_name']==model_name]], ignore_index=True)\n",
    "        df_csv = df_csv.reset_index(drop=True)\n",
    "        # print(df_csv[df_csv['file_name']=='a01_sykkel.wav'])\n",
    "        \n",
    "        if len(df_csv.model_name.unique()) > 1:\n",
    "            print(f\"Error: {path} has more than one model name\")\n",
    "            break\n",
    "        \n",
    "        # Add ID column\n",
    "        df_csv[\"id\"] = df_csv[\"file_name\"].apply(lambda x: x.split('_')[0])\n",
    "        df_csv = df_csv.sort_values(by=['id'])\n",
    "        \n",
    "        # print(df_csv.columns)    \n",
    "        # Reorder column names\n",
    "        reorder_column = [\"id\", \"global_score\", \n",
    "                        \"target_word\", \"PER\", \n",
    "                        \"transcribed_word\", \"CER\", \n",
    "                        \"transcribed_word_striped\", \"striped_CER\",\n",
    "                        \"file_name\", \"model_name\"]\n",
    "        \n",
    "        df_csv = df_csv[reorder_column]\n",
    "        # print(df_csv.columns)   \n",
    "\n",
    "        if bool:\n",
    "            # Save the file\n",
    "            csv_name = path.split('/')[-1]\n",
    "            df_csv.to_csv(os.path.join(results_dir, csv_name), index=False)    \n",
    "\n",
    "# resave_transcription_df(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:#FFC75F\"> BOX PLOT &nbsp;:&nbsp; </span>**\n",
    "#### <span style=\"color:#F9F871\">  Every *model* compared &nbsp;:&nbsp; CER vs. PER for <b>Native</b>, <b>Non Native</b> and <b>All</b> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directory to save the box plots &nbsp; : &nbsp; <span style=\"color:#FF6F91\"> *./BoxPlots/Model_CER*</i></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_box_cer = '../Transcriptions/Results/BoxPlots/Model_CER'\n",
    "if not os.path.exists(save_dir_box_cer):\n",
    "    os.makedirs(save_dir_box_cer)\n",
    "    print(f\"Directory {save_dir_box_cer} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a fuction that works for both all, native and non-native speakers\n",
    "\n",
    "##### **<span style=\"color:#FF9671\"> Giga Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could also use \"lst_csv\", but safer to remake the list\n",
    "lst_csv_re_saved = os.listdir(results_dir)\n",
    "# lst_csv_re_saved = [file for file in lst_csv_re_saved if file.startswith('tran') & file.endswith('v1.csv')] \n",
    "lst_csv_re_saved = [file for file in lst_csv_re_saved if file.startswith('true_tran') & file.endswith('v1.csv')] \n",
    "lst_csv_re_saved = sorted(lst_csv_re_saved, key=lambda x: test_for_models.index(x.split('_')[-2]))\n",
    "lst_path = [os.path.join(results_dir, file) for file in lst_csv_re_saved] \n",
    "ggm = pd.DataFrame()\n",
    "# Iterate through the CSV files to add what is missing\n",
    "for path in lst_path:\n",
    "    # print(f\"Reading {path}\")\n",
    "    df_csv = pd.read_csv(path)\n",
    "    ggm = pd.concat([ggm, df_csv], ignore_index=True)\n",
    "ggm = ggm.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<span style=\"color:#FFC75F\"> Fix model names for Giga Matrix**\n",
    "Fix what the model names in the box plot shows, and that the PER can be one of the boxes in teh box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_model_names(gigamind:pd.DataFrame, cer_value:str = 'CER'):\n",
    "    # Remove \"whisper\" from the model name\n",
    "    gigamind['model_name'] = gigamind['model_name'].apply(lambda x: x.replace('nb-whisper', 'NNL'))\n",
    "\n",
    "    # Get Capitalized model names fro tiny, base and medium\n",
    "    gigamind['model_name'] = gigamind['model_name'].apply(lambda x: x.capitalize() if not x.startswith('NNL') or x.startswith('P') else x)\n",
    "\n",
    "    # Add PER to model names so it gets its own column\n",
    "    new_per = gigamind[gigamind.model_name == 'Tiny'][['PER', 'file_name', 'global_score']].copy()\n",
    "    new_per = new_per.rename(columns={\"PER\": f\"{cer_value}\"})\n",
    "    new_per['model_name'] = 'Phone Error Rate (PER)'\n",
    "\n",
    "    gigamind = pd.concat([new_per, gigamind], ignore_index=True)\n",
    "    return gigamind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<span style=\"color:#F9F871\"> BOX PLOT function for all models comparing PER and CER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plots_model(giga_df:pd.DataFrame, save_dir:str, title:str, cer_value:str = 'CER', cer_name:str = 'CER', \n",
    "                    extended:int=2.1, max_verbatim:int = 1.5, save:bool = True):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # ---------- Color palette  ---------- #\n",
    "    palette = ['#FFC75F', # PER color\n",
    "        '#A8BB5C', '#20887A', '#2F4858', # Model colors       \n",
    "        '#A8BB5C', '#20887A', '#2F4858',          \n",
    "        '#A8BB5C', '#20887A', '#2F4858'\n",
    "        ]\n",
    "    \n",
    "    # ---------- Horizontal lines ---------- #\n",
    "    # Line for the maximum cer_value for the medium-verbaitm\n",
    "    max_cer_model1 = giga_df[(giga_df['model_name'] == 'NNL-medium-verbatim') & \n",
    "                            (giga_df[cer_value] >= 0) & \n",
    "                            (giga_df[cer_value] <= max_verbatim)][cer_value].max()\n",
    "    ax.axhline(max_cer_model1, color='#B5AA99', linestyle='--')\n",
    "    \n",
    "    # Line for the median value\n",
    "    median_value = giga_df[giga_df['model_name'] == 'Phone Error Rate (PER)'][cer_value].median()\n",
    "    ax.axhline(median_value, color='#C1554D', linestyle='--')\n",
    "    \n",
    "    # Add to make space for the legend\n",
    "    ax.axhline(extended, color='#ffffff', linestyle='--')\n",
    "    \n",
    "    #  ----------  Boxplot ---------- #\n",
    "    sns.boxplot(x='model_name', y=cer_value, hue='model_name', data=giga_df, ax=ax, showfliers=False, palette=palette)\n",
    "\n",
    "    #  ---------- Model legend box  ---------- #  (Custom legends found by ChatGPT) \n",
    "    chosen_colors = ['#A8BB5C', '#20887A', '#2F4858']\n",
    "    chosen_names = ['OpenAI', 'Norwegian National Library (NNL)', 'NNL-Verbatim']\n",
    "    legend_patches = [mpatches.Patch(color=color, label=name) for name, color in zip(chosen_names, chosen_colors)]\n",
    "\n",
    "    # Add legend with custom settings\n",
    "    # legend = ax.legend(handles=legend_patches, loc='upper right', frameon=True, shadow=True, title='Models')\n",
    "    # legend.get_frame().set_facecolor('#f0eeeb')  # Background color\n",
    "    # legend.get_title().set_fontsize('12')  # Title font size\n",
    "    \n",
    "    legend = ax.legend(handles=legend_patches, loc='upper right', frameon=False, shadow=False)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize('14')  # Legend text font size\n",
    "    \n",
    "    #  ---------- Title and labels ---------- #\n",
    "    plt.title(f'PER compared to CER  - {title} - {cer_name}', fontsize=21)\n",
    "    plt.xlabel('Target words  &  Model names', fontsize=17)\n",
    "    plt.ylabel(f'PER & CER', fontsize=17)\n",
    "    \n",
    "    #  ---------- Wrap x-axis ---------- # (Wrap method found by ChatGPT) \n",
    "    # model_names = [ '\\n'.join(wrap(label, 9)) for label in giga_df['model_name'].unique() ]\n",
    "    model_names = [\n",
    "    '\\n'.join(wrap(label, 8)) if label.endswith((\"(PER)\")) or label.startswith((\"NNL\")) else label \n",
    "    for label in giga_df['model_name'].unique()]\n",
    "    \n",
    "    plt.xticks(ticks=range(len(model_names)), labels=model_names, fontsize=15)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    # ------- Save or Show ---------- #\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(save_dir)\n",
    "        plt.close()\n",
    "    else: \n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><span style=\"color:#FF6F91\"> Box plot for all models, all data, native and non-native</span></b> Unmodified (U) *NOT STRIPED (NS)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ All data Box Plot ------------ #\n",
    "ggm_fix = fix_model_names(ggm)\n",
    "box_plots_model(ggm_fix, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_All_Data_NS.png'), 'Whole dataset',\n",
    "                cer_name = 'Unmodified Transcription (UT)')\n",
    "\n",
    "# -------------- Non-Native Box Plot -------------- #\n",
    "ggm_d = ggm[ggm['id'].str.contains('d')].copy()\n",
    "ggm_fix_non_native = fix_model_names(ggm_d)\n",
    "box_plots_model(ggm_fix_non_native, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_Native_NS.png'), \n",
    "                            title='Native dataset', extended = 2.2, max_verbatim = 1,\n",
    "                            cer_name = 'Unmodified Transcription (UT)')\n",
    "\n",
    "# -------------- Non Native Box Plot -------------- #\n",
    "ggm_a = ggm[ggm['id'].str.contains('a')].copy()\n",
    "ggm_fix_native = fix_model_names(ggm_a)\n",
    "box_plots_model(ggm_fix_native, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_Non_Native_Data_NS.png'), \n",
    "                            title='Non Native dataset', extended = 2.05, max_verbatim = 1.6,\n",
    "                            cer_name = 'Unmodified Transcription (UT)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><span style=\"color:#FF6F91\"> Box plot for all models, all data, native and non-native</span></b> Modified (M) *STRIPED (S)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ All data Box Plot ------------ #\n",
    "ggm_fix_S = fix_model_names(ggm, cer_value='striped_CER')\n",
    "box_plots_model(ggm_fix_S, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_All_Data_S.png'), \n",
    "                            title = 'Whole dataset', cer_value = 'striped_CER', cer_name = 'Modified Transcription (MT)',\n",
    "                            extended = 2, max_verbatim = 1.4)\n",
    "\n",
    "\n",
    "# -------------- Native Box Plot -------------- #\n",
    "ggm_d = ggm[ggm['id'].str.contains('d')].copy()\n",
    "ggm_fix_native_S = fix_model_names(ggm_d, cer_value='striped_CER')\n",
    "box_plots_model(ggm_fix_native_S, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_Native_Data_S.png'), \n",
    "                            title='Native dataset', extended = 2.2, max_verbatim = 1,\n",
    "                            cer_value = 'striped_CER', cer_name = 'Modified Transcription (MT)')\n",
    "\n",
    "# -------------- Non-Native Box Plot -------------- #\n",
    "ggm_a = ggm[ggm['id'].str.contains('a')].copy()\n",
    "ggm_fix_non_native_S = fix_model_names(ggm_a, cer_value='striped_CER')\n",
    "box_plots_model(ggm_fix_non_native_S, os.path.join(save_dir_box_cer, 'PER_&_CER_All_Models_Non_Native_S.png'), \n",
    "                            title='Non Native dataset', extended = 2.1, max_verbatim = 1.6,\n",
    "                            cer_value = 'striped_CER', cer_name = 'Modified Transcription (MT)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FFC75F\">  Every **ID** compared for best model &nbsp;:&nbsp; CER vs. PER for <b>Native</b>, <b>Non Native</b> and <b>all</b> </span>\n",
    "Create directory to save the box plots &nbsp; : &nbsp; <span style=\"color:#F9F871\"> *./BoxPlots/Model_CER*</i></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<span style=\"color:#D65DB1\"> BOX PLOT function for all ID's comparing PER and CER** ➝ Only use the best model including **<span style=\"color:#D65DB1\"> Modified (M) / Unmodified (U) *Striped/Non Striped*</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plots_id(best_model:pd.DataFrame, save_dir:str, cer:str = 'CER', cer_name:str = 'CER', save:bool = False):\n",
    "    # ------ Melt model, so PER and CER can be compared ------ #\n",
    "    df_melted = pd.melt(best_model, id_vars=['id'], value_vars=['PER', cer],\n",
    "                        var_name='cer_type', value_name='cer_value') \n",
    "    df_melted_a = df_melted[df_melted['id'].str.contains('a')].copy() # Non Native\n",
    "    df_melted_d = df_melted[df_melted['id'].str.contains('d')].copy() # Native \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(25, 6))\n",
    "    #  ----------  Boxplot ---------- #\n",
    "    sns.boxplot(x='id', y='cer_value', hue='cer_type', data=df_melted_a, ax=ax, showfliers=False, palette=['#FFC75F', '#00966E'], linewidth=1.5)\n",
    "    sns.boxplot(x='id', y='cer_value', hue='cer_type', data=df_melted_d, ax=ax, showfliers=False, palette=['#FFC75F', '#C1554D'], linewidth=1.5)\n",
    "    \n",
    "    #  ---------- Title and labels ---------- #\n",
    "    plt.title(f\"PER for target words compared to CER for the different ID's - {cer_name}\", fontsize=25)\n",
    "    plt.xlabel(\"PER Target words &  CER ID's\", fontsize=18)\n",
    "    plt.ylabel(f'PER & CER', fontsize=18)\n",
    "    \n",
    "    #  ---------- x- and y-axis ---------- #\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    #  ---------- Model legend box  ---------- # \n",
    "    chosen_colors = ['#FFC75F', '#00966E', '#C1554D']\n",
    "    chosen_names = ['PER', 'CER Non Native', 'CER Native']\n",
    "    legend_patches = [mpatches.Patch(color=color, label=name) for name, color in zip(chosen_names, chosen_colors)]\n",
    "\n",
    "    # Add legend with custom settings\n",
    "    # legend = ax.legend(handles=legend_patches, loc='upper left', frameon=True, shadow=True)\n",
    "    legend = ax.legend(handles=legend_patches, loc='upper left', frameon=False, shadow=False)\n",
    "    legend.get_frame().set_facecolor('#f0eeeb')  # Background color\n",
    "    # legend.get_title().set_fontsize('16')  # Title font size\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize('16')  # Legend text font size\n",
    "    \n",
    "    \n",
    "    #  ---------- Save or Show ---------- #\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(save_dir)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the <span style=\"color:#FF9671\">**BEST MODEL**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- BEST MODEL -------- #\n",
    "model_path = '../Transcriptions/Results/transcriptions_nb-whisper-medium-verbatim_v1.csv'\n",
    "df_best = pd.read_csv(model_path)\n",
    "best_color = '#2f4858'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><span style=\"color:#FFC75F\"> Box plot for all id's and all data</span></b> 'Unmodified (U) *NOT STRIPED (NS)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plots_id(df_best, os.path.join(save_dir_box_cer, 'PER_&_CER_All_IDs_NS.png'), cer='CER', cer_name='Unmodified Transcription (UT)', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b><span style=\"color:#FFC75F\"> Box plot for all id's and all data</span></b> Modified (M) *STRIPED (S)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plots_id(df_best, os.path.join(save_dir_box_cer, 'PER_&_CER_All_IDs_S.png'), cer='striped_CER', cer_name='Modified Transcription (MT)', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#845EC2\"> **BOX PLOT** for **Score** groups  and CER / PER </span> ➝ Only use the best model **Unmodified** *NOT STRIPED (NS)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#D65DB1\"> Quartile Calcuations: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box**: The solid block of the boxplot represents the interquartile range (IQR), which is the range between the first quartile (Q1) and the third quartile (Q3). The ends of the box are:\n",
    "\n",
    "**Lower end**(bottom of the box): The first quartile (Q1), which is the median of the lower half of the data.\n",
    "\n",
    "**Upper end** (top of the box): The third quartile (Q3), which is the median of the upper half of the data.\n",
    "\n",
    "We use the Quartile calculations to determine the global score segments for the transcribed words. This first 2 models uses CER and striped_CER. \n",
    "The last plot where all models are plotted together, the PER is used instead of CER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- Calculate the quartiles ---------- #\n",
    "q1 = df_best.groupby('global_score')['CER'].quantile(0.25)\n",
    "q3 = df_best.groupby('global_score')['CER'].quantile(0.75)\n",
    "# print(f\"Q1:\\n{q1}\")\n",
    "# print(f\"Q3:\\n{q3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF6F91\"> BOX PLOTS: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Melt model, so PER and CER can be compared ------ #\n",
    "df_melted_score = pd.melt(df_best, id_vars=['global_score'], value_vars=['PER', 'CER'],\n",
    "                    var_name='cer_type', value_name='cer_value') \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "colors = ['#FFC75F', '#2F4858']\n",
    "\n",
    "# ---------- Horizontal lines ---------- #\n",
    "score_range = ['#d2e7e4', '#a6cfca', '#63aca2', '#20887a', '#1a6d62']\n",
    "q1 = df_best.groupby('global_score')['CER'].quantile(0.25)\n",
    "q3 = df_best.groupby('global_score')['CER'].quantile(0.75)\n",
    "\n",
    "align = 4.68\n",
    "for score in range(1, 6):  # Iterate over scores 1 to 5\n",
    "    if score == 1:\n",
    "        text_y = q3[1]\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    elif score == 5:\n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhline(q1[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q1[score], ymax=q3[score], color=score_range[-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q1[score]+ q3[score])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    else:    \n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q3[score], ymax=q3[score+1], color=score_range[score-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q3[score] + q3[score + 1])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "        \n",
    "#  ----------  Boxplot ---------- #\n",
    "sns.boxplot(x='global_score', y='cer_value', hue='cer_type', data=df_melted_score, ax=ax, showfliers=False, palette=colors, linewidth=1.5)\n",
    "\n",
    "#  ---------- Model legend box  ---------- # \n",
    "chosen_colors = colors\n",
    "chosen_names = ['PER: Target Words', 'CER: NNL-Verbatim']\n",
    "legend_patches = [mpatches.Patch(color=color, label=name) for name, color in zip(chosen_names, chosen_colors)]\n",
    "\n",
    "legend = ax.legend(handles=legend_patches, loc='best', frameon=False, shadow=False)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize('11')  # Legend text font size\n",
    "\n",
    "#  ---------- Title and labels ---------- #\n",
    "plt.title(f'PER and CER for each score for all models compaired - Unmodified Transcription (MT)', fontsize=17)\n",
    "plt.xlabel('Global Score', fontsize=14)\n",
    "plt.ylabel(f'PER & CER', fontsize=14)\n",
    "\n",
    "#  ---------- Wrap x-axis ---------- # (Wrap method found by ChatGPT) \n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# ------- Save or Show ---------- #\n",
    "plt.tight_layout()\n",
    "\n",
    "save_dir = os.path.join(save_dir_box_cer, 'Score_per_cer_best_model_all_data_NS.png')\n",
    "plt.savefig(save_dir)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#D65DB1\"> **BOX PLOT** same as over but </span>➝ **Modified Transcription (MT)** *STRIPED (S)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Melt model, so PER and CER can be compared ------ #\n",
    "df_melted_score = pd.melt(df_best, id_vars=['global_score'], value_vars=['PER', 'striped_CER'],\n",
    "                    var_name='cer_type', value_name='cer_value') \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# ---------- Horizontal lines ---------- #\n",
    "score_range = ['#d2e7e4', '#a6cfca', '#63aca2', '#20887a', '#1a6d62']\n",
    "q1 = df_best.groupby('global_score')['striped_CER'].quantile(0.25)\n",
    "q3 = df_best.groupby('global_score')['striped_CER'].quantile(0.75)\n",
    "\n",
    "align = 4.68\n",
    "for score in range(1, 6):  # Iterate over scores 1 to 5\n",
    "    if score == 1:\n",
    "        text_y = q3[1]\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    elif score == 5:\n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhline(q1[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q1[score], ymax=q3[score], color=score_range[-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q1[score]+ q3[score])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    else:    \n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q3[score], ymax=q3[score+1], color=score_range[score-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q3[score] + q3[score + 1])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "        \n",
    "#  ----------  Boxplot ---------- #\n",
    "sns.boxplot(x='global_score', y='cer_value', hue='cer_type', data=df_melted_score, ax=ax, showfliers=False, palette=['#FFC75F', best_color], linewidth=1.5)\n",
    "\n",
    "#  ---------- Model legend box  ---------- # \n",
    "chosen_colors = ['#FFC75F', best_color]\n",
    "chosen_names = ['PER: Target Words', 'CER: NNL-Verbatim']\n",
    "legend_patches = [mpatches.Patch(color=color, label=name) for name, color in zip(chosen_names, chosen_colors)]\n",
    "\n",
    "legend = ax.legend(handles=legend_patches, loc='best', frameon=False, shadow=False)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize('11')  # Legend text font size\n",
    "\n",
    "#  ---------- Title and labels ---------- #\n",
    "plt.title(f'PER and CER for each score for all models compaired - Modified Transcription (MT)', fontsize=17)\n",
    "plt.xlabel('Global Score', fontsize=14)\n",
    "plt.ylabel(f'PER & CER', fontsize=14)\n",
    "\n",
    "#  ---------- Wrap x-axis ---------- # (Wrap method found by ChatGPT) \n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# ------- Save or Show ---------- #\n",
    "plt.tight_layout()\n",
    "\n",
    "save_dir = os.path.join(save_dir_box_cer, 'Score_per_cer_best_model_all_data_S.png')\n",
    "plt.savefig(save_dir)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#FF6F91\"> **BOX PLOT** for **Score** groups  and CER / PER </span> ➝ For all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggm_fix = fix_model_names(ggm)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "# ---------- Color palette  ---------- #\n",
    "palette = ['#FFC75F', # PER color\n",
    "            '#A8BB5C', '#20887A', '#2F4858', # Model colors       \n",
    "            '#A8BB5C', '#20887A', '#2F4858',          \n",
    "            '#A8BB5C', '#20887A', '#2F4858']\n",
    "\n",
    "# ---------- Horizontal lines ---------- #\n",
    "score_range = ['#d2e7e4', '#a6cfca', '#63aca2', '#20887a', '#1a6d62']\n",
    "q1 = ggm_fix[ggm_fix['model_name']=='Phone Error Rate (PER)'].groupby('global_score')['CER'].quantile(0.25)\n",
    "q3 = ggm_fix[ggm_fix['model_name']=='Phone Error Rate (PER)'].groupby('global_score')['CER'].quantile(0.75)\n",
    "\n",
    "align = 4.68\n",
    "for score in range(1, 6):  # Iterate over scores 1 to 5\n",
    "    if score == 1:\n",
    "        text_y = q3[1]\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    elif score == 5:\n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhline(q1[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q1[score], ymax=q3[score], color=score_range[-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q1[score]+ q3[score])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "    else:    \n",
    "        ax.axhline(q3[score], color='#C1554D', linestyle='--')\n",
    "        ax.axhspan(ymin=q3[score], ymax=q3[score+1], color=score_range[score-1], alpha=0.5)\n",
    "        \n",
    "        text_y = (q3[score] + q3[score + 1])/ 2\n",
    "        ax.text(align, text_y, f'Score {score}', va='center', ha='center', color=score_range[-1], fontsize=12)\n",
    "        \n",
    "\n",
    "#  ----------  Boxplot ---------- #\n",
    "sns.boxplot(x='global_score', y='CER', hue='model_name', data=ggm_fix, ax=ax, showfliers=False, palette=palette)\n",
    "\n",
    "#  ---------- Model legend box  ---------- #  (Custom legends found by ChatGPT) \n",
    "chosen_colors = ['#FFC75F', '#A8BB5C', '#2F4858', '#20887A']\n",
    "chosen_names = ['PER', 'OpenAI', 'NNL-Verbatim', 'Norwegian National Library (NNL)']\n",
    "legend_patches = [mpatches.Patch(color=color, label=name) for name, color in zip(chosen_names, chosen_colors)]\n",
    "\n",
    "\n",
    "legend = ax.legend(handles=legend_patches, loc='best', frameon=False, shadow=False, ncol = 2)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize('11')  # Legend text font size\n",
    "\n",
    "#  ---------- Title and labels ---------- #\n",
    "plt.title(f'PER and CER for each score for all models compaired ', fontsize=17)\n",
    "plt.xlabel('Global Score', fontsize=14)\n",
    "plt.ylabel(f'PER & CER', fontsize=14)\n",
    "\n",
    "#  ---------- Wrap x-axis ---------- # (Wrap method found by ChatGPT) \n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# ------- Save or Show ---------- #\n",
    "plt.tight_layout()\n",
    "\n",
    "save_dir = os.path.join(save_dir_box_cer, 'Score_per_cer_all_models_all_data_NS.png')\n",
    "plt.savefig(save_dir)\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
