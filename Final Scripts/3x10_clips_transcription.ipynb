{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><span style=\"color:#ADF88B\">10 x 3 sec audio files to 30 sec audio file </span> </b>\n",
    "\n",
    "This script concatenates 10x3sec audio files into one 30sec audio file. \n",
    "The script uses the <span style=\"color:#ADF88B\"><b> pydub library</b> </span> to manipulate the audio files. \n",
    "\n",
    "\n",
    "## <span style=\"color:#58F2B3\"> <b>Method: </b> Concatenated Clips  </span>\n",
    "\n",
    "An ID column containing the assessment information was added to the data frame. \n",
    "\n",
    "\n",
    "It was then sorted and grouped by the ID column. The audio clips in each group were then concatenated to equally 10x3 audio clips. \n",
    "\n",
    "\n",
    "All the concatenated audio clips were saved to new directories for easy access.\n",
    "\n",
    "\n",
    "\n",
    "Each audio clip is 3 seconds long, while Whisper expects audio clips of 30 seconds or longer. Therefore, Whisper's responses were tested using 30-second audio clips, achieved by concatenating ten 3-second audio clips. Each clip does not correlate, but it might reveal valuable results for Whisper's performance.\n",
    "\n",
    "The Verbatim Tiny and Medium models from the Norwegian National Library (NNL) were used to compare the performance differences between the two, which had a high difference in features. \n",
    "\n",
    "Multiple tests were conducted to evaluate Whisper's response in this scenario:\n",
    "\n",
    "1. <span style=\"color:#00E6DB\"> Grouping by score: Each score was grouped before transcription. Sorted by file name.</span>\n",
    "2. <span style=\"color:#00E6DB\"> Grouping by score: Each score was grouped before transcription. Mixed file names.</span>\n",
    "3. <span style=\"color:#00D5F7\"> Grouping by person: Each person's independent words were grouped, and sorted by score.</span>\n",
    "4. <span style=\"color:#00D5F7\"> Random shuffling of data before transcription.</span>\n",
    "\n",
    "\n",
    "The results were analyzed using <span style=\"color:#F9F871\">**Word Error Rate (WER)**</span> and Character Error Rate (CER) scores to assess the performance of the models.\n",
    "\n",
    "If looking for outliers this could have proved usefull, considering if the transcription where much longer og contained question marks.\n",
    "\n",
    "<span style=\"color:#00C0FF\">#00C0FF </span>\n",
    "<span style=\"color:#00D5F7\">#00D5F7 </span>\n",
    "<span style=\"color:#00E6DB\">#00E6DB </span>\n",
    "<span style=\"color:#58F2B3\">#58F2B3 </span>\n",
    "<span style=\"color:#ADF88B\">#ADF88B </span>\n",
    "<span style=\"color:#F9F871\">#F9F871 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ADF88B\"> <b>Library imports </b> </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys      \n",
    "script_directory = '../'\n",
    "sys.path.append(script_directory)\n",
    "import self_made_functions as smf\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment # Good for audio manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import whisper\n",
    "import jiwer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#58F2B3\"> <b> Concatination information functions </b> </span>\n",
    "\n",
    "\n",
    "Save and extract relevant information from the audio files.\n",
    "\n",
    "The CER and WER was not calculated correctly fro the first batch of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Part 1 - Generate the audio files ----------- #\n",
    "def concat_audio(df:pd.DataFrame, save_directory:str, s:int=0, e:int=10, wv_path:str = '/talebase/data/speech_raw/teflon_no/speech16khz/', name = ''):\n",
    "    concat_rows = df.iloc[s:e]  # Rows to concatenate\n",
    "    input_lst = list(concat_rows.Word.values) # get input string\n",
    "    \n",
    "    combined_audio = AudioSegment.empty() # Empty audio file to concatenate the audio clips\n",
    "    id_list = [] # List of the speaker IDs\n",
    "    \n",
    "    target_wer_count = 0\n",
    "    pron_lst = [] # list of pron scores to calcualte CER\n",
    "    \n",
    "    for row in concat_rows.itertuples():\n",
    "        id_list.append(row.id.split('_')[0])\n",
    "        \n",
    "        audio = AudioSegment.from_file(os.path.join(wv_path, row[1]))\n",
    "        combined_audio += audio\n",
    "        cer_score = (row.pronScores.count('1')/len(row.pronScores.split(' ')))\n",
    "        \n",
    "        pron_lst.append(row.pronScores)\n",
    "        if cer_score != 0:\n",
    "            target_wer_count += 1   \n",
    "    \n",
    "    target_wer = target_wer_count/len(input_lst) # Correct Word Error Rate (WER)\n",
    "    \n",
    "    pron_lst = (' '.join(pron_lst)) # String of all the 0 adn 1s\n",
    "    target_cer = pron_lst.count('1')/len(pron_lst) # Correct Character Error Rate (CER)\n",
    "    \n",
    "    audio_name = name + f'words_{s}_{e}.wav'\n",
    "    # print('in funttion', audio_name)\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        print(f\"Directory {save_directory} created\")\n",
    "\n",
    "    # For store filer! Ikke lagre dem direkte i git repo, men på serveren\n",
    "    audio_directory = os.path.join(save_directory, audio_name)\n",
    "    # Save the combined audio\n",
    "    if not os.path.exists(audio_directory):\n",
    "        combined_audio.export(audio_directory, format=\"wav\")   \n",
    "    \n",
    "    return  audio_name, id_list, input_lst, target_cer, target_wer\n",
    "\n",
    "# Create dir if not exist\n",
    "def new_dir(dir_name): \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        print(f\"Directory {dir_name} created\")\n",
    "    else:\n",
    "        print(f\"Directory {dir_name} already exists\")\n",
    "\n",
    "# -------------- Part 2 - Look at the result -------------- #\n",
    "def get_transcribed_words(string_words:str):\n",
    "    # Return a list with lower case words and no dots\n",
    "    split_string = string_words.split(' ')\n",
    "    remove_empty = [word for word in split_string if word != '']\n",
    "    lower = [word.lower() for word in remove_empty] \n",
    "    no_dot = [word.replace('.', '') for word in lower]\n",
    "    return no_dot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin, wv_path = smf.get_correct_df()\n",
    "# Speaker ID column added\n",
    "df_trail =  df_fin\n",
    "df_trail['id'] = df_fin['File name'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>Score</th>\n",
       "      <th>Prosody</th>\n",
       "      <th>Noise/Disruption</th>\n",
       "      <th>Pre-speech noise</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Word</th>\n",
       "      <th>Pronunciation</th>\n",
       "      <th>pronScores</th>\n",
       "      <th>Assessor</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01_artist.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>A t` I s t</td>\n",
       "      <td>1 0 1 1 1</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01_barn.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>barn</td>\n",
       "      <td>b A: n`</td>\n",
       "      <td>1 0 1</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01_bart.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bart</td>\n",
       "      <td>b A t`</td>\n",
       "      <td>1 1 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01_bjoern.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bjørn</td>\n",
       "      <td>b j 2: n`</td>\n",
       "      <td>1 1 0 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01_blaa.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blå</td>\n",
       "      <td>b l o:</td>\n",
       "      <td>1 1 0</td>\n",
       "      <td>Anne Marte</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File name  Score  Prosody  Noise/Disruption  Pre-speech noise  \\\n",
       "0  a01_artist.wav      4      NaN               NaN               NaN   \n",
       "1    a01_barn.wav      3      1.0               NaN               NaN   \n",
       "2    a01_bart.wav      4      NaN               NaN               NaN   \n",
       "3  a01_bjoern.wav      3      NaN               NaN               NaN   \n",
       "4    a01_blaa.wav      4      NaN               NaN               NaN   \n",
       "\n",
       "   Repetition    Word Pronunciation pronScores    Assessor   id  \n",
       "0         NaN  artist    A t` I s t  1 0 1 1 1  Anne Marte  a01  \n",
       "1         NaN    barn       b A: n`      1 0 1  Anne Marte  a01  \n",
       "2         NaN    bart        b A t`      1 1 0  Anne Marte  a01  \n",
       "3         NaN   bjørn     b j 2: n`    1 1 0 0  Anne Marte  a01  \n",
       "4         NaN     blå        b l o:      1 1 0  Anne Marte  a01  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trail.head()\n",
    "# cer = (df_trail.pronScores[0].count('1')/len(df_trail.pronScores[0].split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00E6DB\"> <b>1. Grouping by score:</b> Each score was grouped before transcription. Sorted by speaker</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_score_sort(model_name):\n",
    "        # Group by score, sort by speaker ID\n",
    "        df_trail_1 = df_trail.sort_values(by=['id'], ascending=False) # Sorted by ID\n",
    "        df_trail_1 = df_trail_1.reset_index(drop=True)\n",
    "        df_score_groups = df_trail_1.groupby('Score')# Grouped by score\n",
    "\n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "                'score', 'input_string', 'translated_string',\n",
    "                'translated_CER', 'translated_WER', 'target_CER',  'target_WER',\n",
    "                \"unique_id's\", \"speaker_id's\",\n",
    "                'length_deviation_words',\n",
    "                'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../../3x10_Concatenations' # Save outside of git repo\n",
    "\n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "        for idx, score_group in df_score_groups:\n",
    "                # if idx > 1:\n",
    "                #         print('out at', idx)\n",
    "                #         break\n",
    "                print(idx)\n",
    "        # -------------- Make or Find directory -------------- #\n",
    "                score_directory = f'{ground_folder}/3x10_score_{idx}_sorted'\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(score_group)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(score_group, score_directory, s, e)\n",
    "                        unique_ids = list(set(ids))\n",
    "\n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(score_directory,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                                        # print(input_words_list[i], translation_list[i], cer) \n",
    "                                        # wer = jiwer.wer(input_words_list[i], translation_list[i])\n",
    "                                        # trans_wer.append(wer)\n",
    "                                # print(input_str)\n",
    "                                # print(translation['text'])\n",
    "                                # print('CER:', trans_cer)\n",
    "                                # print('target CER', target_cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        new_row = {'audio_path': [score_directory],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [idx],\n",
    "                                'unique_id\\'s': [unique_ids],\n",
    "                                'speaker_id\\'s': [ids],\n",
    "                                \n",
    "                                'target_CER' : [target_cer],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'translated_CER':[trans_cer], \n",
    "                                'translated_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = '../3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_scores_id_sorted'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00E6DB\"> <b>2. Grouping by score:</b> Each score was grouped before transcription. Randomized speakers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_score_mixed(model_name):\n",
    "        # Group by score, sort by speaker ID\n",
    "        df_trail_2 = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "        df_score_groups = df_trail_2.groupby('Score')# Grouped by score\n",
    "        \n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "                'score', 'input_string', 'translated_string',\n",
    "                'translated_CER', 'translated_WER', 'target_CER',  'target_WER',\n",
    "                \"unique_id's\", \"speaker_id's\",\n",
    "                'length_deviation_words',\n",
    "                'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../../3x10_Concatenations' # Save outside of git repo\n",
    "\n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "        for idx, score_group in df_score_groups:\n",
    "                # if idx > 1:\n",
    "                #         print('out at', idx)\n",
    "                #         break\n",
    "                print(idx)\n",
    "        # -------------- Make or Find directory -------------- #\n",
    "                score_directory = f'{ground_folder}/3x10_score_{idx}_mixed'\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(score_group)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(score_group, score_directory, s, e)\n",
    "                        unique_ids = list(set(ids))\n",
    "                        \n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(score_directory,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                                        # print(input_words_list[i], translation_list[i], cer) \n",
    "                                        # wer = jiwer.wer(input_words_list[i], translation_list[i])\n",
    "                                        # trans_wer.append(wer)\n",
    "                                # print(input_str)\n",
    "                                # print(translation['text'])\n",
    "                                # print('CER:', trans_cer)\n",
    "                                # print('target CER', target_cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        new_row = {'audio_path': [score_directory],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [idx],\n",
    "                                'unique_id\\'s': [unique_ids],\n",
    "                                'speaker_id\\'s': [ids],\n",
    "                                'target_CER' : [target_cer],\n",
    "                                #[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'translated_CER' :[trans_cer], \n",
    "                                'translated_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = '../3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_scores_id_mixed'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00D5F7\"><b> 3. Grouping by person:</b> Each person's independent words were grouped, and sorted by score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_id(model_name): \n",
    "        df_name_groups = df_trail.groupby('id')  # Grouped by file name\n",
    "\n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "        \"speaker_id\", 'score', 'input_string', 'translated_string',\n",
    "        'translated_CER', 'translated_WER', 'target_CER',  'target_WER',\n",
    "\n",
    "        'length_deviation_words',\n",
    "        'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../../3x10_Concatenations/3x10_id_sorted' # Save outside of git repo\n",
    "        \n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "        for idx, person in df_name_groups:\n",
    "                # print(person.iloc[0]['id'])\n",
    "                # if person.iloc[0]['id'] == 'a06':\n",
    "                #         break\n",
    "        # --------- Concatenate and save audio files --------- #\n",
    "                for i in range(0, len(person)//10,10):\n",
    "                        # Concatenate the 10 audio files together\n",
    "                        s, e = i, i + 10 # get start and end index\n",
    "                        speaker_id = person.iloc[0]['id']\n",
    "                        audio_name, _, input_words_list, target_cer, target_wer = concat_audio(person, ground_folder, s, e, name=f'{speaker_id}_')\n",
    "                        # print(audio_name)\n",
    "                        \n",
    "        # ---------- Transcribe the audio file ---------- #\n",
    "                        load_audio = whisper.load_audio(os.path.join(ground_folder,audio_name))\n",
    "                        translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                        \n",
    "                        # calcualte deviation in str length\n",
    "                        translation_list = get_transcribed_words(translation['text'])\n",
    "                        length_deviation = len(translation_list) - len(input_words_list)\n",
    "                        \n",
    "                        # Calculate cer and wer with jiwter\n",
    "                        input_str = ' '.join(input_words_list)\n",
    "                        translation_str = ' '.join(translation_list)\n",
    "                        # print(input_str)\n",
    "                        # print(translation['text'])\n",
    "                        # print(translation_str)\n",
    "                        trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                        trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                        \n",
    "                        if len(input_words_list) == len(translation_list):\n",
    "                                trans_cer = []\n",
    "                                for i in range(len(input_words_list)):\n",
    "                                        cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                        trans_cer.append(cer)\n",
    "                        \n",
    "        # --------- Store information in data frame --------- #\n",
    "                        # add to dataframe\n",
    "                        scores = person.iloc[s:e]['Score'].values\n",
    "                        \n",
    "                        new_row = {'audio_path': [ground_folder],\n",
    "                                'audio_name': [audio_name],\n",
    "                                \n",
    "                                'score': [scores],\n",
    "                                'speaker_id': [speaker_id],\n",
    "                                'target_CER' : [target_cer],\n",
    "                                #[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                                'input_string': [input_str],\n",
    "                                \n",
    "                                'translated_CER' :[trans_cer], \n",
    "                                'translated_WER':[trans_wer], \n",
    "                                'translated_string': [translation_str],# [translation],\n",
    "                                \n",
    "                                'length_deviation_words' : [length_deviation]\n",
    "                        }\n",
    "                        new_row = pd.DataFrame(new_row, index=[0])\n",
    "                        # print(new_row.columns.values)\n",
    "                        # print(new_row)\n",
    "                        concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                        \n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = '../3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_group_id_sorted_score'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        print(new_csv_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00D5F7\"><b> 4. Random shuffling of data before transcription.</span></b >\n",
    "\n",
    "Persons not sorted, and results not grouped by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffel_concat(model_name):\n",
    "        # Group by file name, sort by speaker ID\n",
    "        df_no_group = df_trail.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        concatenated_audio_information = pd.DataFrame(columns=[\n",
    "        'score', 'input_string', 'translated_string',\n",
    "        'translated_CER', 'translated_WER', 'target_CER',  'target_WER',\n",
    "        \"unique_id's\", \"speaker_id's\",\n",
    "        'length_deviation_words',\n",
    "        'audio_name', 'audio_path'])\n",
    "\n",
    "        ground_folder = '../../3x10_Concatenations/3x10_no_group_mixed' # Save outside of git repo\n",
    "        \n",
    "        # model_name = 'nb-whisper-tiny-verbatim'\n",
    "        model_path = smf.get_whisper_path(model_name)\n",
    "        model = pipeline(\"automatic-speech-recognition\", model_path)\n",
    "\n",
    "# --------- Concatenate and save audio files --------- #\n",
    "        for i in range(0, len(df_no_group)//10,10):\n",
    "                # Concatenate the 10 audio files together\n",
    "                s, e = i, i + 10 # get start and end index\n",
    "                audio_name, ids, input_words_list, target_cer, target_wer = concat_audio(df_no_group, ground_folder, s, e)\n",
    "                unique_ids = list(set(ids))\n",
    "                \n",
    "# ---------- Transcribe the audio file ---------- #\n",
    "                load_audio = whisper.load_audio(os.path.join(ground_folder,audio_name))\n",
    "                translation = model(load_audio, generate_kwargs={'task': 'transcribe', 'language': 'no'}) \n",
    "                \n",
    "                # calcualte deviation in str length\n",
    "                translation_list = get_transcribed_words(translation['text'])\n",
    "                length_deviation = len(translation_list) - len(input_words_list)\n",
    "                \n",
    "                # Calculate cer and wer with jiwter\n",
    "                input_str = ' '.join(input_words_list)\n",
    "                translation_str = ' '.join(translation_list)\n",
    "                # print(input_str)\n",
    "                # print(translation['text'])\n",
    "                # print(translation_str)\n",
    "                trans_cer = jiwer.cer(input_str, translation_str)\n",
    "                trans_wer = jiwer.wer(input_str, translation_str)\n",
    "                \n",
    "                if len(input_words_list) == len(translation_list):\n",
    "                        trans_cer = []\n",
    "                        for i in range(len(input_words_list)):\n",
    "                                cer = jiwer.cer(input_words_list[i], translation_list[i])\n",
    "                                trans_cer.append(cer)\n",
    "                \n",
    "# --------- Store information in data frame --------- #\n",
    "                # add to dataframe\n",
    "                scores = df_no_group.iloc[s:e]['Score'].values\n",
    "                \n",
    "                new_row = {'audio_path': [ground_folder],\n",
    "                        'audio_name': [audio_name],\n",
    "                        \n",
    "                        'score': [scores],\n",
    "                        'unique_id\\'s': [unique_ids],\n",
    "                        'speaker_id\\'s': [ids],\n",
    "                        'target_CER' : [target_cer],\n",
    "                        #[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                        'target_WER' : [target_wer],#[score_group.iloc[s,e]['Score'].values[0]],\n",
    "                        'input_string': [input_str],\n",
    "                        \n",
    "                        'translated_CER' :[trans_cer], \n",
    "                        'translated_WER':[trans_wer], \n",
    "                        'translated_string': [translation_str],# [translation],\n",
    "                        \n",
    "                        'length_deviation_words' : [length_deviation]\n",
    "                }\n",
    "                new_row = pd.DataFrame(new_row, index=[0])\n",
    "                # print(new_row.columns.values)\n",
    "                concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n",
    "                \n",
    "        # ----- Save the concatenated audio information ----- #\n",
    "        csv_folder = '../3x10_Concatenation_information'\n",
    "        base_name = f'{model_name}__concatenated_audio_information_no_group_mixed'\n",
    "        new_dir(csv_folder)\n",
    "        new_csv_name, _  = smf.get_new_csv_name(csv_folder, base_name)\n",
    "        concatenated_audio_information.to_csv(new_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#58F2B3\">  **Run** every test for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3586713/1691793832.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Directory ../3x10_Concatenation_information already exists\n",
      "../3x10_Concatenation_information/tiny__concatenated_audio_information_scores_id_sorted_v1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3586713/3559032620.py:84: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concatenated_audio_information = pd.concat([concatenated_audio_information, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# # Test 1 for all the models\n",
    "test_for_models = ['tiny', 'nb-whisper-tiny', 'nb-whisper-tiny-verbatim',\n",
    "                    'base', 'nb-whisper-base', 'nb-whisper-base-verbatim',\n",
    "                    'medium', 'nb-whisper-medium', 'nb-whisper-medium-verbatim']\n",
    "for model_name in test_for_models:\n",
    "    print(model_name)\n",
    "    \n",
    "    run_for_score_sort(model_name) \n",
    "    run_for_score_mixed(model_name) \n",
    "    group_by_id(model_name)\n",
    "    random_shuffel_concat(model_name)\n",
    "# # Kjør gjennom alle csv filene laget og sammen like på vhem som kalrer flest like lange transcribsjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F9F871\"> Old Code</span>\n",
    "\n",
    "Only tried for global score 5, but for multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test show good results for one example for the base,  nb-whisper-medium and  nb-whisper-medium-verbatim.\n",
    "\n",
    "Considering the  nb-whisper-medium-verbatim showed promising results for the other metrics I will try concatenating the rest of teh 5 results for this model.\n",
    "\n",
    "Then calculate the success rate, and deciding if the rest of the scores should be found from this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ser vi at den kan i noen tilfeller transkribere rikgit antall ord, og riktig transcribering uten om noen.\n",
    "\n",
    "Men i fleteparten av tilfellene er det feil antall ord, og feil transcribering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! clip 22 -> 210_220 har en lengre setting \n",
    "\n",
    "Clip 20 med 23 ulike transcriberinger har ikke noen ekstra ord.\n",
    "\n",
    "whisper base vireker helt vilt dårlig, så ser at nb sine modeller er bedre her i alle fall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_amanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
